---
layout: post
title: Self-Study Note - Optimal and Learning-Based Control (AA 203)
date: 2024-12-16 11:12:00-0400
description: My PhD preparatory course
tags: math
categories: note
thumbnail: assets/img/blog/optcontrol/PID_en.svg.png
related_posts: false
published: true
---

> The following are my self-study notes for the PhD preparatory course, Optimal and Learning-Based Control (AA 203). 
This content is intended solely for personal review and to motivate myself.
Please email me at <12011126@mail.sustech.edu.cn> to report any issue, and the original course metarials are at <https://stanfordasl.github.io/aa203/sp2223/>.


### Content

1. [Lec 01](#lec_01) Introduction
2. [Lec 02](#lec_02) Nonlinear optimization theory

 
<p id="lec_01"></p>              

# Lecture 01 - Introduction

Click [here](https://stanfordasl.github.io/aa203/sp2223/pdfs/lecture/lecture_1.pdf) to visit the online slides.

## 1.1 Dynamical System

For a continuous process, we have: 
Time $$t\in R$$, State $$x(t)\in R^n$$, Control Input $$u(t)\in R^m$$.
Dynamics $$\dot{x}(t) = f (t,x(t), u(t))$$, and the Trajectories $$x:t\mapsto  x(t), u:u\mapsto  u(t)$$, where $$f$$ is sufficiently for each initial condition.

It can be written in matrix format, more generally, for a multi dimensions circumstance, it is

$$
 \left(  \begin{matrix}
   \dot{s} \\ \dot{v}
  \end{matrix} \right) = 
  \left[ \begin{matrix} 0 & I \\0 & 0 \\ \end{matrix} \right] \left(  \begin{matrix}
   s \\ v
  \end{matrix} \right)+ 
  \left[  \begin{matrix}
   0 \\ I
  \end{matrix} \right] u
$$

The Proportional-derivative (PD) feedback is represented as:

$$
u = -k_ps-k_dv \Rightarrow \left(  \begin{matrix}
   \dot{s} \\ \dot{v}
  \end{matrix} \right) = \left[ \begin{matrix} 0 & I \\ -k_p & -k_d \\ \end{matrix} \right] \left(  \begin{matrix}
   s \\ v
  \end{matrix} \right)
$$


**PID/PD:**
A common feedback-based control loop mechanism commonly used to manage machines and processes and process that require continuous control and automatic adjustment.
- The proportional (P) component responds to the current error value by producing an output that is directly proportional to the magnitude of the error.
-  The integral (I) component, in turn, considers the cumulative sum of past errors to address any residual steady-state errors that persist over time, eliminating lingering discrepancies.
- The derivative (D) component predicts future error by assessing the rate of change of the error, which helps to mitigate overshoot and enhance system stability, particularly when the system undergoes rapid changes.

The PID controller reduces the likelihood of human error and improves automation.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/optcontrol/PID_en.svg.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    A block diagram of a PID controller in a feedback loop. r(t) is the desired process variable (PV) or setpoint (SP), and y(t) is the measured PV.
</div>

## 1.2 Stability and Lyapunov Function

### 1.2.1 The mathematical definitions of Stability

For a projection $$\dot{x}=f(x)$$, and an equilibrium point $$\bar{x}\in R^n$$, the stability of a system means the following, where the parameters are compact values.

1.) Marginal/Lyapunov: Trajectories that start close to the equilibrium remain close to the equilibrium.

$$\forall\epsilon>0,\exists\delta>0: \|x(0) - \bar{x}\|<\delta \Rightarrow \|x(t)-\bar{x}\|<\epsilon,\forall t\ge0$$

2.) Asymptotic (local): Trajectories that start near the equilibrium converge to it.

$$\exists\delta>0:\|x(0)-\bar{x}\|<\delta \Rightarrow \lim_{t\to\infty}\|x(t)-\bar{x}\|=0$$

3.) Exponential (local): Trajectories that start near the equilibrium converge to it exponentially fast.

$$\exists\delta,c,\alpha>0:\|x(0)-\bar{x}\|<\delta\Rightarrow\|x(t)-\bar{x}\|\le ce^{-\alpha t}\|x(0)-\bar{x}\|$$

Take the compact value $$\delta\to\infty$$ to get the global definitions. For Linear Time-Invariant (LTI) system, "Asymptotic = Exponential" and "local = global" always.

### 1.2.2 Lyapunov Function

1.) Lipschitz continuity

For a function on real set $$f:D\in R\to R$$, it can be represented as $$\|f(a)-f(b)\|\le K\|a-b\|, \forall a,b\in D$$.Moreover, given two metric spaces $$(X, d_X)$$ and $$(Y, d_Y)$$, where $$d_X$$ denotes the metric on the set $$X$$ and $$d_Y$$ is the metric on set $$Y$$, a function $$f:X\to Y$$ is called Lipschitz continuous if there exists a rea; constant $$K\ge 0$$ such that, for all $$x_1, x_2 \in X$$

$$d_Y(f(x_1),f(x_2))\le K d_X(x_1,x_2)$$

Any such $$K$$ is referred to as a Lipschitz constant for the function $$f$$ and $$f$$ may also be referred to as K-Lipschitz. The smallest constant is sometimes called the (best) Lipschitz constant of $$f$$ for the dilation or dilatation of $$f$$. 

2.) Lyapunov function & directly method

The function which equilibrium point at $$x=0$$, used to describe a time-invariant/autonomous system.

Consider a continuous scalar function $$V(x)$$ that is 0 at the origin and positive ($$V(x)\ge 0, V(x)=0\Leftrightarrow x=0$$) elsewhere in some ball enclosing the origin. What's more, the$$\dot{V}$$ is negative definite, like $$\nabla V(x)^Tf(x)\le 0, \nabla V(x)^Tf(x)=0 \Leftrightarrow x=0$$. 

Then $$\bar{x}=0$$ is the locally asymptotically stable, $$V(0)$$ is the equilibrium point. In addition, if $$V$$ is radially unbounded and $$V(x)\to\infty$$ as $$\|x\|\to\infty$$, then $$\bar{x}=0$$ is globally asymptotically stable as well.

3.) Converse Lyapunov theorem

It is the converse statement of Lyapunov directly method. If $$\bar{x}=0$$ is the locally asymptotically stable equilibrium, with the region of attraction $$\mathcal{A}\subset R^n$$, then for $$V\in \mathcal{C}^1(R^n,R)$$ such that:

1. The $$V$$ is positive definite on $$\mathcal{A}$$
2. The $$\dot{V}$$ is negative-definite on $$\mathcal{A}$$
3. The $$V(x)\to \infty$$ as $$x\to\partial\mathcal{A} $$
4. The $$\{x\|V(x)\le c\}$$ is a compact subset of $$\mathcal{A}$$ for any $$c>0$$.
5. If $$\bar{x}=0$$ is globally equilibrium, then $$V$$ is radially unbounded.


## 1.3 Optimal Control Problem

Basically, it is a optimal process, with objective functions subject to convex or concave constraints. In control field, it can be represented as the follow.

For a continuous-time process, the objective/cost function (terminal+stage) is

$$\min_{x,u} J(x,u) := \ell_T(T,x(T)) + \int_{0}^{T} \ell(t,x(t),u(t)) dt $$

which subject to the following constraints:

- dynamical feasibility: $$\dot{x}=f(t,x(t),u(t)),\forall t\in[0,T]$$
- boundary conditions: $$x(t_0)=x_0, x(T)\in\mathcal{X}_T$$
- state constraints: $$x(t)\in\mathcal{X}, \forall t\in[0,T]$$
- input constraints: $$u(t)\in\mathcal{U}, \forall t\in[0,T]$$

There's one difference, 
- Open-loop input: $$u^*(t)$$ for specific initial state $$x_0$$
- Closed-loop input: $$u^*(t)=\pi^*(t,x(t))$$

[Back to content](#content)         
 
<p id="lec_02"></p>              

# Lecture 02 - Nonlinear optimization theory

Click [here](https://stanfordasl.github.io/aa203/sp2223/pdfs/lecture/lecture_2.pdf) to visit the online slides.

## 2.1 Unconstrained Optimization

- First order necessary condition: let $$x^*$$ to be a local minimum, then the near point, we must have

$$f(x^*+\delta x) - f(x^*) \simeq \nabla f(x^*)^T\Delta x\ge 0$$



[Back to content](#content)
