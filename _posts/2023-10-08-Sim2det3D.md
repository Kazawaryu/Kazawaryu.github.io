---
layout: post
title: 3D Traffic Vulnerable Group Detection in Simulation-driven Autonomous Driving
date: 2023-10-08 11:12:00-0400
description: Machine learning course work
tags: AD
categories: sample-posts
related_posts: false
published: true
---

# Introduction

Autonomous driving is one of the most exciting topics in the fields of machine learning and deep learning. In recent years, the technology behind autonomous driving has advanced rapidly in both academia and industry. The various aspects of autonomous driving can be divided into three main modules: perception, prediction, and decision-making. Due to the data-driven nature of deep learning models, effective algorithms require high-quality data sets. If these high standards are not met, the desired outcomes are unlikely to be achieved. Currently, it is acknowledged that in the decision-making module, training on 1 million kilometers of data can lead to better results. However, no similar benchmark exists for the perception module.

Previous 3D Lidar detection algorithms often overlooked vulnerable traffic groups, such as pedestrians and cyclists. We aim to propose a straightforward strategy or framework that maximizes the utility of unit data frames within enhanced datasets, thereby improving the algorithm's detection of vulnerable traffic groups.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/sim2det/framework.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    The simple main pipeline of this project
</div>


Unfortunately, Lidar sensors are expensive, making them unaffordable for our project team. Furthermore, collecting and annotating Lidar data poses significant challenges. Compared to traditional data or image data for autonomous driving, Lidar data is often “too sparse, abstract in description, and difficult to visualize.” This is especially true for identifying traffic-disadvantaged groups, as labeling them based on real data is particularly challenging.

To address these issues, our project employs a simulation environment for data collection and preparation. We have developed a set of tools to automatically collect and annotate data within this simulation environment, which allows us to directly extract the 3D positions of targets for annotation. Additionally, given the low frequency of vulnerable groups in existing datasets, we constructed simple scenarios in high dimensions to increase the sample size of these groups.

In summary, to tackle the challenges posed by the small and unclear samples of vulnerable traffic groups (pedestrians and cyclists) in traditional datasets, our project has built custom collection tools based on a simulation environment. We also proposed a strategic framework to enhance data collection. To verify its effectiveness, we tested our custom dataset across multiple algorithms, observing a significant improvement in the detection of vulnerable traffic groups.

# Related Work

CARLA is a powerful open-source simulator designed for autonomous driving research. It can create a virtual urban environment and simulate various sensors, including cameras, LiDAR, and mmWave radar, to provide essential data. Many researchers have developed their self-driving systems within the Carla environment. By utilizing established object detection algorithms such as YOLO and Faster R-CNN to process the data generated by Carla, they can effectively implement object detection in their systems. The same applies to object tracking, where algorithms like GOTURN and Deep SORT can be employed to achieve successful tracking.

For this project, we will use the open-source 3D point cloud detection algorithm training framework, OpenPCDet. This framework is currently a popular and lightweight option for point cloud algorithm training and supports a variety of network architectures.

| **Model**                      | **Car@R11** | **Pedestrian@R11** | **Cyclist@R11** | **Dataset** |
|--------------------------------|-------------|--------------------|-----------------|-------------|
| PointPillar  | 77.28       | 52.29              | 62.68           | KITTI       |
| SECOND            | 78.62       | 52.98              | 67.15           | KITTI       |
| Voxel R-CNN    | 84.54       | -                  | -               | KITTI       |
| BEVFusion      | 67.75       | -                  | -               | nuScenes    |
| CenterPoint  | 78.08       | 49.74              | 67.22           | ONCE        |
| Voxel NeXt     | 30.05       | -                  | -               | Argoverse2  |

Among these, we chose the PointPillar model for experiments to verify that our method has improved the detection effect of if groups. Voxel R-CNN was selected for experiments to verify that the traditional vehicle detection effect has also been improved.

# Methodology

## Co-simulation

This section primarily focuses on creating the simulation scene. In the Carla map, there are numerous static vehicles that are integral to the map design and are not generated through program control. Consequently, their 3D positions do not appear in memory and cannot be annotated. During the training and verification phases, the model may detect these static vehicles but might misclassify them due to the lack of annotations.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/sim2det/ue41.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/sim2det/ue42.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Operate source code version Carla in the Unreal4
</div>

Therefore, we first operate in the source version of Carla and use the Unreal4 toolkit to eliminate static vehicles. After that, we used the Carla-Apollo Bridge to let Apollo take over Carla's dynamic scene settings and perform visual operations in DreamViewer.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/sim2det/cosim1.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blog/sim2det/cosim2.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Operate source code version Carla in the Unreal4
</div>


## Problem Definition

Given a 3D point cloud, $\mathbf{P}=\{p_1,p_2,...,p_n\}$, which represents the set of measured points, $p_i=(x_i,y_i,z_i)$, and presents a snapshots of the surroundings. For objects, $\mathbf{O}=\{o_1,o_2,...,o_m\}$, represents the set of all objects in point cloud (vehicle, traffic light, pedestrian, etc.), using KITTI format, where $o_j=(x_j,y_j,z_j,w_j,h_j,l_j,r_j)$.

Use $S_{keep}$ to measure whether to keep the Lidar data.
$$S_{keep} \equiv \mathcal{E} (P) \cdot \mathcal{P}(\rho(O),\tau(O))$$
where $\mathcal{E}$ decides whether to keep the entire point cloud, and $\mathcal{P}$ decides whether to keep the collected target data.

Use $S_{value}$ to represent the value of current Lidar data for the trainning model,
$$S_{value}\equiv \phi(P, O) \cdot \psi(O)$$
where $\phi(P, O)$ is the perception distance term, used to describe the relationship between the effective perception radius and the farthest perception radius; $\psi(O)$ is the prediction accuracy term, used to describe the perception How accurate is the prediction of objects within the radius.



