---
layout: post
title: 3D Traffic Vulnerable Group Detection in Simulation-driven Autonomous Driving
date: 2023-10-08 11:12:00-0400
description: Machine learning course work
tags: AD
categories: sample-posts
related_posts: false
published: true
---

# Abstract

The perception link is the first crucial step in the autonomous driving process. Earlier 3D point cloud perception algorithms and datasets often focused on enhancing the performance indicators of the algorithms while neglecting the detection of small targets, such as vulnerable groups in traffic. To address this issue, creating high-quality datasets for these disadvantaged groups is essential. This article introduces a new joint simulation strategy that combines Carla and Apollo to enhance data quality and improve model performance. The strategy was tested on the PointPillar algorithm and the Voxel R-CNN algorithm, yielding the desired results.

# Introduction

Autonomous driving is one of the most exciting topics in the fields of machine learning and deep learning. In recent years, the technology behind autonomous driving has advanced rapidly in both academia and industry. The various aspects of autonomous driving can be divided into three main modules: perception, prediction, and decision-making. Due to the data-driven nature of deep learning models, effective algorithms require high-quality data sets. If these high standards are not met, the desired outcomes are unlikely to be achieved. Currently, it is acknowledged that in the decision-making module, training on 1 million kilometers of data can lead to better results. However, no similar benchmark exists for the perception module.

Previous 3D Lidar detection algorithms often overlooked vulnerable traffic groups, such as pedestrians and cyclists. We aim to propose a straightforward strategy or framework that maximizes the utility of unit data frames within enhanced datasets, thereby improving the algorithm's detection of vulnerable traffic groups.

Unfortunately, Lidar sensors are expensive, making them unaffordable for our project team. Furthermore, collecting and annotating Lidar data poses significant challenges. Compared to traditional data or image data for autonomous driving, Lidar data is often “too sparse, abstract in description, and difficult to visualize.” This is especially true for identifying traffic-disadvantaged groups, as labeling them based on real data is particularly challenging.

To address these issues, our project employs a simulation environment for data collection and preparation. We have developed a set of tools to automatically collect and annotate data within this simulation environment, which allows us to directly extract the 3D positions of targets for annotation. Additionally, given the low frequency of vulnerable groups in existing datasets, we constructed simple scenarios in high dimensions to increase the sample size of these groups.

In summary, to tackle the challenges posed by the small and unclear samples of vulnerable traffic groups (pedestrians and cyclists) in traditional datasets, our project has built custom collection tools based on a simulation environment. We also proposed a strategic framework to enhance data collection. To verify its effectiveness, we tested our custom dataset across multiple algorithms, observing a significant improvement in the detection of vulnerable traffic groups.

# Related Work

CARLA is a powerful open-source simulator designed for autonomous driving research. It can create a virtual urban environment and simulate various sensors, including cameras, LiDAR, and mmWave radar, to provide essential data. Many researchers have developed their self-driving systems within the Carla environment. By utilizing established object detection algorithms such as YOLO and Faster R-CNN to process the data generated by Carla, they can effectively implement object detection in their systems. The same applies to object tracking, where algorithms like GOTURN and Deep SORT can be employed to achieve successful tracking.

For this project, we will use the open-source 3D point cloud detection algorithm training framework, OpenPCDet. This framework is currently a popular and lightweight option for point cloud algorithm training and supports a variety of network architectures.

| **Model**                      | **Car@R11** | **Pedestrian@R11** | **Cyclist@R11** | **Dataset** |
|--------------------------------|-------------|--------------------|-----------------|-------------|
| PointPillar  | 77.28       | 52.29              | 62.68           | KITTI       |
| SECOND            | 78.62       | 52.98              | 67.15           | KITTI       |
| Voxel R-CNN    | 84.54       | -                  | -               | KITTI       |
| BEVFusion      | 67.75       | -                  | -               | nuScenes    |
| CenterPoint  | 78.08       | 49.74              | 67.22           | ONCE        |
| Voxel NeXt     | 30.05       | -                  | -               | Argoverse2  |

Among these, we chose the PointPillar model for experiments to verify that our method has improved the detection effect of if groups. Voxel R-CNN was selected for experiments to verify that the traditional vehicle detection effect has also been improved.

# Methodology

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/9.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/7.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

