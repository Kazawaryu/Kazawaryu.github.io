<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://jonaruthardt.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jonaruthardt.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-19T05:33:38+00:00</updated><id>https://jonaruthardt.github.io/feed.xml</id><title type="html">blank</title><subtitle>Academic website of Zexuan Jia. </subtitle><entry><title type="html">DeepSeek Application Integrations</title><link href="https://jonaruthardt.github.io/blog/2025/ds/" rel="alternate" type="text/html" title="DeepSeek Application Integrations"/><published>2025-02-02T15:12:00+00:00</published><updated>2025-02-02T15:12:00+00:00</updated><id>https://jonaruthardt.github.io/blog/2025/ds</id><content type="html" xml:base="https://jonaruthardt.github.io/blog/2025/ds/"><![CDATA[<p>&lt;/br&gt; &lt;/br&gt;</p> <h3 id="applications">Applications</h3> <table> <tr> <td> <img src="https://avatars.githubusercontent.com/u/171659527?s=400&amp;u=39906ab3b6e2066f83046096a66a77fb3f8bb836&amp;v=4" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/quantalogic/quantalogic">Quantalogic</a> </td> <td> QuantaLogic is a ReAct (Reasoning &amp; Action) framework for building advanced AI agents. </td> </tr> <tr> <td> <img src="https://github.com/deepseek-ai/awesome-deepseek-integration/assets/13600976/224d547a-6fbc-47c8-859f-aa14813e2b0f" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/chatbox/README.md">Chatbox</a> </td> <td> Chatbox is a desktop client for multiple cutting-edge LLM models, available on Windows, Mac and Linux. </td> </tr> <tr> <td> <img src="https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/bb65404c-f867-42d8-ae2b-281fe953ab54" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/chatgpt_next_web/README.md"> ChatGPT-Next-Web </a> </td> <td> ChatGPT Next Web is a cross-platform ChatGPT web UI, with GPT3, GPT4 &amp; Gemini Pro support. </td> </tr> <tr> <td> <img src="./docs/Coco AI/assets/favicon.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/Coco AI/README.md">Coco AI</a></td> <td> <a href="https://coco.rs">Coco AI</a> is a fully open-source, cross-platform unified search and productivity tool that connects and searches across various data sources, including applications, files, Google Drive, Notion, Yuque, Hugo, and more, both local and cloud-based. By integrating with large models like DeepSeek, Coco AI enables intelligent personal knowledge management, emphasizing privacy and supporting private deployment, helping users quickly and intelligently access their information. </td> </tr> <tr> <td> <img src="./docs/liubai/assets/liubai-logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/liubai/README.md">Liubai</a> </td> <td> Liubai allows DeepSeek to have arms and legs to manipulate your notes, tasks, calendars, and to-do lists just on WeChat! </td> </tr> <tr> <td> <img src="https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/1ac9791b-87f7-41d9-9282-a70698344e1d" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/pal/README.md"> Pal - AI Chat Client<br/>(iOS, ipadOS) </a> </td> <td> Pal is a customized chat playground on iOS. </td> </tr> <tr> <td> <img src="https://www.librechat.ai/librechat.svg" alt="LibreChat" width="64" height="auto"/> </td> <td> <a href="https://www.librechat.ai/docs/configuration/librechat_yaml/ai_endpoints/deepseek">LibreChat</a> </td> <td> LibreChat is a customizable open-source app that seamlessly integrates DeepSeek for enhanced AI interactions. </td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/longevity-genie/chat-ui/11c6647c83f9d2de21180b552474ac5ffcf53980/static/geneticsgenie/icon-128x128.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/longevity-genie/just-chat">Just-Chat</a> </td> <td> Make your LLM agent and chat with it simple and fast!</td> </tr> <tr> <td> <img src="https://www.papersgpt.com/images/logo/favicon.ico" alt="PapersGPT" width="64" height="auto"/> </td> <td> <a href="https://github.com/papersgpt/papersgpt-for-zotero">PapersGPT</a> </td> <td> PapersGPT is a Zotero plugin that seamlessly with DeepSeek and other multiple AI models for quickly reading papers in Zotero. </td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/rss-translator/RSS-Translator/main/core/static/favicon.ico" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/rss_translator/README.md"> RSS Translator </a> </td> <td> Translate RSS feeds into your language! </td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/ysnows/enconvo_media/main/logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/enconvo/README.md"> Enconvo </a> </td> <td> Enconvo is the Launcher of the AI era, the entry point for all AI functions, and a thoughtful intelligent assistant.</td> </tr> <tr> <td><img src="https://github.com/kangfenmao/cherry-studio/blob/main/src/renderer/src/assets/images/logo.png?raw=true" alt="Icon" width="64" height="auto" style="border-radius: 10px"/></td> <td><a href="docs/cherrystudio/README.md">Cherry Studio</a></td> <td>A powerful desktop AI assistant for producer</td> </tr> <tr> <td> <img src="https://tomemo.top/images/logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/tomemo/README.md"> ToMemo (iOS, ipadOS) </a> </td> <td> A phrasebook + clipboard history + keyboard iOS app with integrated AI macromodeling for quick output use in the keyboard.</td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/buxuku/video-subtitle-master/refs/heads/main/resources/icon.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/buxuku/video-subtitle-master">Video Subtitle Master</a></td> <td> Batch generate subtitles for videos, with the ability to translate subtitles into other languages. This is a client-side tool that supports both Mac and Windows platforms and integrates with multiple translation services such as Baidu, Volcengine, DeepLx, OpenAI, DeepSeek, and Ollama.</td> </tr> <tr> <td> <img src="https://github.com/UnknownEnergy/chatgpt-api/blob/master/dist/assets/chatworm-72x72.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/UnknownEnergy/chatgpt-api/blob/master/README.md">Chatworm</a> </td> <td> Chatworm is a webapp for multiple cutting-edge LLM models, open-source and also available on Android. </td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/tisfeng/Easydict">Easydict</a></td> <td> Easydict is a concise and easy-to-use translation dictionary macOS App that allows you to easily and elegantly look up words or translate text. Supports calling large language model APIs for translation.</td> </tr> <tr> <td> <img src="https://www.raycast.com/favicon-production.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/raycast/README.md">Raycast</a></td> <td> <a href="https://raycast.com/?via=ViGeng">Raycast</a> is a productivity tool for macOS that lets you control your tools with a few keystrokes. It supports various extensions including DeepSeek AI.</td> </tr> &lt;/tr&gt; <td> <img src="https://niceprompt.app/favicon.ico" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://niceprompt.app">Nice Prompt</a></td> <td> <a href="https://niceprompt.app">Nice Prompt</a> Organize, share and use your prompts in your code editor, with Cursor and VSCode。</td> &lt;/tr&gt; <tr> <td> <img src="https://avatars.githubusercontent.com/u/193405629?s=200&amp;v=4" alt="PHP Client" width="64" height="auto"/> </td> <td> <a href="https://github.com/deepseek-php/deepseek-php-client/blob/master/README.md">PHP Client</a> </td> <td> Deepseek PHP Client is a robust and community-driven PHP client library for seamless integration with the Deepseek API. </td> </tr> <tr> <tr> <td> <img src="https://github.com/tornikegomareli/DeepSwiftSeek/blob/main/logo.webp" alt="DeepSwiftSeek Logo" width="64" height="auto"/> </td> <td> <a href="https://github.com/tornikegomareli/DeepSwiftSeek/blob/main/README.md">DeepSwiftSeek</a> </td> <td> DeepSwiftSeek is a lightweight yet powerful Swift client library, pretty good integration with the DeepSeek API. It provides easy-to-use Swift concurrency for chat, streaming, FIM (Fill-in-the-Middle) completions, and more. </td> </tr> <td> <img src="https://avatars.githubusercontent.com/u/958072?s=200&amp;v=4" alt="Laravel Integration" width="64" height="auto"/> </td> <td> <a href="https://github.com/deepseek-php/deepseek-laravel/blob/master/README.md">Laravel Integration</a> </td> <td> Laravel wrapper for Deepseek PHP client, to seamless deepseek API integration with laravel applications.</td> </tr> <tr> <td> <img src="./docs/zotero/assets/zotero-icon.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/zotero/README_cn.md">Zotero</a></td> <td> <a href="https://www.zotero.org">Zotero</a> is a free, easy-to-use tool to help you collect, organize, annotate, cite, and share research.</td> </tr> <tr> <td> <img src="https://b3log.org/images/brand/siyuan-128.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/SiYuan/README.md">SiYuan</a> </td> <td> SiYuan is a privacy-first personal knowledge management system that supports complete offline usage, as well as end-to-end encrypted data sync.</td> </tr> <tr> <td> <img src="https://github.com/ArvinLovegood/go-stock/raw/master/build/appicon.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/ArvinLovegood/go-stock/blob/master/README.md">go-stock</a> </td> <td>go-stock is a Chinese stock data viewer built by Wails with NativeUI and powered by LLM.</td> </tr> <tr> <td> <img src="https://avatars.githubusercontent.com/u/102771702?s=200&amp;v=4" alt="Wordware" width="64" height="auto"/> </td> <td> <a href="docs/wordware/README.md">Wordware</a> </td> <td><a href="https://www.wordware.ai/">Wordware</a> is a toolkit that enables anyone to build, iterate, and deploy their AI stack with just natural language.</td> </tr> <tr> <td> <img src="https://framerusercontent.com/images/xRJ6vNo9mUYeVNxt0KITXCXEuSk.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/langgenius/dify/">Dify</a> </td> <td> <a href="https://dify.ai/">Dify</a> is an LLM application development platform that supports DeepSeek models for creating assistants, workflows, text generators, and more. </td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/enricoros/big-AGI/refs/heads/v2-dev/public/favicon.ico" alt="Big-AGI" width="64" height="auto"/> </td> <td> <a href="https://github.com/enricoros/big-AGI/blob/v2-dev/README.md">Big-AGI</a> </td> <td><a href="https://big-agi.com/">Big-AGI</a> is a groundbreaking AI suite designed to democratize access to advanced artificial intelligence for everyone.</td> </tr> <tr> <td> <img src="https://github.com/LiberSonora/LiberSonora/blob/main/assets/avatar.jpeg?raw=true" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/LiberSonora/LiberSonora/blob/main/README_en.md">LiberSonora</a> </td> <td> LiberSonora, meaning "Voice of Freedom", is an AI-powered, robust, open-source audiobook toolkit that includes features like intelligent subtitle extraction, AI title generation, multilingual translation, with support for GPU acceleration and batch offline processing.</td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/ripperhe/Bob/master/docs/_media/icon_128.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://bobtranslate.com/">Bob</a></td> <td> <a href="https://bobtranslate.com/">Bob</a> is a macOS translation &amp; OCR tool ready to use in any app — right out of the box!</td> </tr> <tr> <td> <img src="https://agenticflow.ai/favicon.ico" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://agenticflow.ai/">AgenticFlow</a> </td> <td> <a href="https://agenticflow.ai/">AgenticFlow</a> is a no-code platform where marketers build agentic AI workflows for go-to-market automation, powered by hundreds of everyday apps as tools for your AI agents.</td> </tr> <tr> <td> <img src="https://github.com/ZGGSONG/STranslate/raw/main/img/favicon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://stranslate.zggsong.com/en/">STranslate</a></td> <td> <a href="https://stranslate.zggsong.com/en/">STranslate</a>（Windows） is a ready-to-go translation ocr tool developed by WPF </td> </tr> <tr> <td> <img src="https://github.com/user-attachments/assets/5e16beb0-993e-47bf-807e-7c8804b313a2" alt="Asp Client" width="64" height="auto"/> </td> <td> <a href="https://github.com/Anwar-alhitar/Deepseek.Asp.Client/blob/master/README.md">ASP Client</a> </td> <td><a href="https://github.com/Anwar-alhitar/Deepseek.Asp.Client/blob/master/README.md">Deepseek.ASPClient</a> is a lightweight ASP.NET wrapper for the Deepseek AI API, designed to simplify AI-driven text processing in .NET applications.. </td> </tr> <tr> <td> <img src="https://www.gptaiflow.tech/logo.png" alt="gpt-ai-flow-logo" width="64" height="auto"/> </td> <td> <a href="https://www.gptaiflow.tech/docs/product/api-keys-setup#setup-deepseek-api-keys">GPT AI Flow</a></td> <td> The ultimate productivity weapon built by engineers for efficiency enthusiasts (themselves): <a href="https://www.gptaiflow.tech/">GPT AI Flow</a> <ul> <li>`Shift+Alt+Space` Wake up desktop intelligent hub</li> <li>Local encrypted storage</li> <li>Custom instruction engine</li> <li>On-demand calling without subscription bundling</li> </ul> </td> </tr> <tr> <td> <img src="https://github.com/user-attachments/assets/b09f17a8-936d-4dac-8b24-1682d52c9a3c" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/alecm20/story-flicks">Story-Flicks</a></td> <td>With just one sentence, you can quickly generate high-definition story short videos, supporting models such as DeepSeek.</td> </tr> <tr> <td> <img src="https://prompt.16x.engineer/favicon.ico" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/16x_prompt/README.md">16x Prompt</a> </td> <td> <a href="https://prompt.16x.engineer/">16x Prompt</a> is an AI coding tool with context management. It helps developers manage source code context and craft prompts for complex coding tasks on existing codebases.</td> </tr> <tr> <td> <img src="https://docs.xark-argo.com/img/logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://www.xark-argo.com">argo</a> </td> <td>Locally download and run Ollama and Huggingface models with RAG on Mac/Windows/Linux. Support LLM API too.</td> </tr> <tr> <td> <img src="https://www.petercat.ai/images/favicon.ico" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://www.petercat.ai">PeterCat</a> </td> <td> A conversational Q&amp;A agent configuration system, self-hosted deployment solutions, and a convenient all-in-one application SDK, allowing you to create intelligent Q&amp;A bots for your GitHub repositories.</td> </tr> <tr> <td> <img src="./docs/ruzhiai_note/assets/play_store_512.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/ruzhiai_note/README.md">RuZhi AI Notes</a> </td> <td>RuZhi AI Notes is an intelligent knowledge management tool powered by AI, providing one-stop knowledge management and application services including AI search &amp; exploration, AI results to notes conversion, note management &amp; organization, knowledge presentation &amp; sharing. Integrated with DeepSeek model to provide more stable and higher quality outputs.</td> </tr> <tr> <td> <img src="https://cdn.link-ai.tech/doc/CoW%20logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/zhayujie/chatgpt-on-wechat">Chatgpt-on-Wechat</a> </td> <td> Chatgpt-on-Wechat(CoW) is a flexible chatbot framework that supports seamless integration of multiple LLMs, including DeepSeek, OpenAI, Claude, Qwen, and others, into commonly used platforms or office software such as WeChat Official Accounts, WeCom, Feishu, DingTalk, and websites. It also supports a wide range of custom plugins. </td> </tr> <tr> <td> <img src="https://athenalab.ai/assets/favicon/favicon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://athenalab.ai/">Athena</a> </td> <td>The world's first autonomous general AI with advanced cognitive architecture and human-like reasoning capabilities, designed to tackle complex real-world challenges.</td> </tr> </table> <h3 id="ai-agent-frameworks">AI Agent frameworks</h3> <table> <tr> <td> <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/mascot_smol.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/huggingface/smolagents/tree/main"> smolagents </a> </td> <td> The simplest way to build great agents. Agents write python code to call tools and orchestrate other agents. Priority support for open models like DeepSeek-R1! </td> </tr> <tr> <td><img src="https://yomo.run/yomo-logo.png" alt="Icon" width="64" height="auto"/></td> <td><a href="docs/yomo/README.md">YoMo</a></td> <td>Stateful Serverless LLM Function Calling Framework with Strongly-typed Language Support</td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/superagentxai/superagentX/refs/heads/master/docs/logo/icononly_transparent_nobuffer.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/superagentx/README.md">SuperAgentX</a> </td> <td>SuperAgentX: A Lightweight Open Source AI Framework Built for Autonomous Multi-Agent Applications with Artificial General Intelligence (AGI) Capabilities.</td> </tr> <tr> <td> <img src="https://panda.fans/_assets/favicons/apple-touch-icon.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/anda/README.md">Anda</a> </td> <td>A Rust framework for AI agent development, designed to build a highly composable, autonomous, and perpetually memorizing network of AI agents.</td> </tr> <tr> <td> <img src="https://rig.rs/assets/favicon.png" alt="Rig (Rust)" width="64" height="auto"/> </td> <td> <a href="https://rig.rs/)](https://rig.rs/">RIG</a> </td> <td>Build modular and scalable LLM Applications in Rust.</td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/longevity-genie/chat-ui/11c6647c83f9d2de21180b552474ac5ffcf53980/static/geneticsgenie/icon-128x128.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/longevity-genie/just-agents">Just-Agents</a> </td> <td>A lightweight, straightforward library for LLM agents - no over-engineering, just simplicity!</td> </tr> <tr> <td> <img src="https://alice.fun/alice-logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/bob-robert-ai/bob/blob/main/alice/readme.md">Alice</a> </td> <td>An autonomous AI agent on ICP, leveraging LLMs like DeepSeek for on-chain decision-making. Alice combines real-time data analysis with a playful personality to manage tokens, mine BOB, and govern ecosystems.</td> </tr> <tr> <td> <img src="https://github.com/Upsonic/Upsonic/blob/9d2e6d43b44defc6744817330625661ca3a2184e/Upsonic%20pp.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/Upsonic/Upsonic">Upsonic</a> </td> <td>Upsonic offers a cutting-edge enterprise-ready agent framework where you can orchestrate LLM calls, agents, and computer use to complete tasks cost-effectively.</td> </tr> <tr> <td> <img src="https://avatars.githubusercontent.com/u/173022229" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/APRO-com">ATTPs</a> </td> <td>A foundational protocol framework for trusted communication between agents. Any agents based on DeepSeek, By integrating with the <a href="https://docs.apro.com/attps">ATTPs</a> SDK, can access features such as agent registration, sending verifiable data, and retrieving verifiable data. So that it can make trusted communication with agents from other platforms. </td> </tr> </table> <h3 id="rag-frameworks">RAG frameworks</h3> <table> <tr> <td> <img src="https://github.com/deepseek-ai/awesome-deepseek-integration/assets/33142505/77093e84-9f7c-4716-9168-bac962fa1372" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/ragflow/README.md"> RAGFlow </a> </td> <td> An open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data. </td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/pingcap/tidb.ai/main/frontend/app/public/nextra/icon-dark.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/autoflow/README.md"> Autoflow </a> </td> <td> <a href="https://github.com/pingcap/autoflow">AutoFlow</a> is an open-source knowledge base tool based on GraphRAG (Graph-based Retrieval-Augmented Generation), built on <a href="https://www.pingcap.com/ai?utm_source=tidb.ai&amp;utm_medium=community">TiDB</a> Vector, LlamaIndex, and DSPy. It provides a Perplexity-like search interface and allows easy integration of AutoFlow's conversational search window into your website by embedding a simple JavaScript snippet. </td> </tr> <tr> <td> <img src="https://assets.zilliz.com/Zilliz_Logo_Mark_White_20230223_041013_86057436cc.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/zilliztech/deep-searcher"> DeepSearcher </a> </td> <td> DeepSearcher combines powerful LLMs (DeepSeek, OpenAI, etc.) and Vector Databases (Milvus, etc.) to perform search, evaluation, and reasoning based on private data, providing highly accurate answer and comprehensive report. </td> </tr> </table> <h3 id="solana-frameworks">Solana frameworks</h3> <table> <tr> <td> <img src="./docs/solana-agent-kit/assets/sendai-logo.png" alt="Icon" width="128" height="auto"/> </td> <td> <a href="docs/solana-agent-kit/README.md"> Solana Agent Kit </a> </td> <td>An open-source toolkit for connecting AI agents to Solana protocols. Now, any agent, using any Deepseek LLM, can autonomously perform 60+ Solana actions: </td> </tr> </table> <h3 id="synthetic-data-curation">Synthetic data curation</h3> <table> <tr> <td> <img src="https://raw.githubusercontent.com/bespokelabsai/curator/main/docs/Bespoke-Labs-Logomark-Red-crop.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/curator/README.md"> Curator </a> </td> <td> An open-source tool to curate large scale datasets for post-training LLMs. </td> </tr> <tr> <td> <img src="https://github.com/user-attachments/assets/8455694b-c52e-40ec-847e-adf6a5ac064f" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/Kiln-AI/Kiln"> Kiln </a> </td> <td>Generate synthetic datasets and distill R1 models into custom fine-tunes. </td> </tr> </table> <h3 id="im-application-plugins">IM Application Plugins</h3> <table> <tr> <td> <img src="https://github.com/InternLM/HuixiangDou/releases/download/v0.1.0rc1/huixiangdou.jpg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/huixiangdou/README_cn.md">HuixiangDou<br/>(wechat,lark)</a> </td> <td>Domain knowledge assistant in personal WeChat and Feishu, focusing on answering questions.</td> </tr> <tr> <td> <img src="https://github.com/RockChinQ/LangBot/blob/master/res/logo.png?raw=true" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/RockChinQ/LangBot">LangBot<br/>（QQ, Lark, WeCom）</a> </td> <td> LLM-based IM bots framework, supports QQ, Lark, WeCom, and more platforms.</td> </tr> <tr> <td> <img src="https://nonebot.dev/logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/KomoriDev/nonebot-plugin-deepseek">NoneBot<br/>（QQ, Lark, Discord, TG, etc.）</a> </td> <td> Based on NoneBot framework, provide intelligent chat and deep thinking functions, supports QQ, Lark, Discord, TG, and more platforms.</td> </tr> </table> <h3 id="browser-extensions">Browser Extensions</h3> <table> <tr> <td> <img src="https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/9d3f42b8-fcd0-47ab-8b06-1dd0554dd80e" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/immersive_translate/README.md"> Immersive Translate </a> </td> <td> Immersive Translate is a bilingual webpage translation plugin. </td> </tr> <tr> <td> <img src="https://lh3.googleusercontent.com/K9i0qJb8phasC5wWf5tU68rhnfvX4swsE0hrhJP-WB3WV7MwE5KpMUIJvHKNHHRE6GKNIvIdTNSWoDMl_NggrmUsaw=s120" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/immersive_reading_guide/README.md"> Immersive Reading Guide </a> </td> <td> NO Sidebar!!! Immersive AI web summarization, ask questions... </td> </tr> <tr> <td> <img src="https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/8a301619-a3de-489b-81fd-69aaa7c1c561" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/chatgpt_box/README.md"> ChatGPT Box </a> </td> <td> ChatGPT Box is a ChatGPT integration in browser, completely for free. </td> </tr> <tr> <td> <img src="https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/c3d9d100-247a-41cc-97c1-10b01ed25e70" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/hcfy/README.md"> hcfy (划词翻译) </a> </td> <td> hcfy (划词翻译) is a web browser extension to integrate multiple translation services. </td> </tr> <tr> <td> <img src="https://static.eudic.net/web/trans/en_trans.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/Lulu Translate/README.md"> Lulu Translate </a> </td> <td> The plugin provides mouse selection translation, paragraph-by-paragraph comparison translation, and PDF document translation functionalities. It can utilize various translation engines, such as DeepSeek AI, Bing, GPT, Google, etc. </td> </tr> <tr> <td> <img src="https://github.com/Bistutu/FluentRead/raw/refs/heads/main/public/icon/192.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://fluent.thinkstu.com/"> FluentRead </a> </td> <td> A revolutionary open-source browser translation plugin that enables everyone to have a native-like reading experience </td> </tr> <tr> <td> <img src="https://www.ncurator.com/_next/image?url=%2Ffavicon.ico&amp;w=96&amp;q=75" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://www.ncurator.com/"> Ncurator </a> </td> <td> Knowledge Base AI Q&amp;A Assistant - Let AI help you organize and analyze knowledge</td> </tr> <tr> <td> <img src="https://github.com/oinzen/RSSFlow-doc/blob/main/docs/images/en/icon64.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://rssflow.oinchain.com"> RssFlow </a> </td> <td>An intelligent RSS reader browser extension with AI-powered RSS summarization and multi-dimensional feed views. Supports DeepSeek model configuration for enhanced content understanding. </td> </tr> <tr> <td> <img src="https://www.typral.com/_next/image?url=%2Ffavicon.ico&amp;w=96&amp;q=75" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://www.typral.com/"> Typral </a> </td> <td> Fast AI writer assistant - Let AI help you quickly improve article, paper, text...</td> </tr> <tr> <td> <img src="https://static.trancy.org/assets/trancy_logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://www.trancy.org/"> Trancy </a> </td> <td>Immersive bilingual translation, video bilingual subtitles, sentence/word selection translation extension</td> </tr> <tr> <td> <img src="https://ziziyi.com/svg/anything_copilot.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/baotlake/anything-copilot"> Anything Copilot </a> </td> <td> Anything Copilot is a browser extension that enables seamless access to mainstream AI tools directly from your sidebar. </td> </tr> </table> <h3 id="vs-code-extensions">VS Code Extensions</h3> <table> <tr> <td> <img src="https://github.com/continuedev/continue/blob/main/docs/static/img/logo.png?raw=true" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/continue/README.md"> Continue </a> </td> <td> Continue is an open-source autopilot in IDE. </td> </tr> <tr> <td> <img src="docs/cline/assets/favicon.png?raw=true" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/cline/README.md"> Cline </a> </td> <td> Meet Cline, an AI assistant that can use your CLI aNd Editor. </td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/Sitoi/ai-commit/refs/heads/main/images/logo.png?raw=true" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/Sitoi/ai-commit/blob/main/README.md"> AI Commit </a> </td> <td> Use AI to generate git commit messages in VS Code. </td> </tr> </table> <h3 id="visual-studio-extensions">Visual Studio Extensions</h3> <table> <tr> <td> <img src="https://merryyellow.gallerycdn.vsassets.io/extensions/merryyellow/comment2gpt/2.0.5/1739475434185/Microsoft.VisualStudio.Services.Icons.Default" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://marketplace.visualstudio.com/items?itemName=MerryYellow.Comment2GPT"> Comment2GPT </a> </td> <td> Use OpenAI ChatGPT, Google Gemini, Anthropic Claude, DeepSeek and Ollama through your comments </td> </tr> <tr> <td> <img src="https://merryyellow.gallerycdn.vsassets.io/extensions/merryyellow/codelens2gpt/2.0.5/1739475875714/Microsoft.VisualStudio.Services.Icons.Default" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://marketplace.visualstudio.com/items?itemName=MerryYellow.CodeLens2GPT"> CodeLens2GPT </a> </td> <td> Use OpenAI ChatGPT, Google Gemini, Anthropic Claude, DeepSeek and Ollama through the CodeLens </td> </tr> <tr> <td> <img src="https://merryyellow.gallerycdn.vsassets.io/extensions/merryyellow/uca-lite/1.4.2/1739392928984/Microsoft.VisualStudio.Services.Icons.Default" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://marketplace.visualstudio.com/items?itemName=MerryYellow.UCA-Lite"> Unity Code Assist Lite </a> </td> <td> Code assistance for Unity scripts </td> </tr> </table> <h3 id="neovim-extensions">neovim Extensions</h3> <table> <tr> <td> <img src="https://github.com/user-attachments/assets/c316f70a-0a3c-4a32-b148-4df15e609acc" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/avante.nvim/README.md"> avante.nvim </a> </td> <td> avante.nvim is an open-source autopilot in IDE. </td> </tr> <tr> <td> <img src="https://github.com/user-attachments/assets/d66dfc62-8e69-4b00-8549-d0158e48e2e0" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/llm.nvim/README.md"> llm.nvim </a> </td> <td> A free large language model (LLM) plugin that allows you to interact with LLM in Neovim. Supports any LLM, such as Deepseek, GPT, GLM, Kimi or local LLMs (such as ollama). </td> </tr> <tr> <td> <img src="https://github.com/user-attachments/assets/d66dfc62-8e69-4b00-8549-d0158e48e2e0" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/codecompanion.nvim/README.md"> codecompanion.nvim </a> </td> <td> AI-powered coding, seamlessly in Neovim. </td> </tr> <tr> <td> <img src="https://github.com/user-attachments/assets/d66dfc62-8e69-4b00-8549-d0158e48e2e0" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/minuet-ai.nvim/README.md"> minuet-ai.nvim </a> </td> <td> Minuet offers code completion as-you-type from popular LLMs including Deepseek, OpenAI, Gemini, Claude, Ollama, Codestral, and more. </td> </tr> </table> <h3 id="jetbrains-extensions">JetBrains Extensions</h3> <table> <tr> <td> <img src="https://plugins.jetbrains.com/files/21520/412905/icon/pluginIcon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://ide.unitmesh.cc/quick-start"> AutoDev </a> </td> <td>‍AutoDev is an open-source AI coding assistant in JetBrain's IDE. </td> </tr> <tr> <td> <img src="https://plugins.jetbrains.com/files/21410/561595/icon/pluginIcon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://plugins.jetbrains.com/plugin/21410-onegai-copilot"> Onegai Copilot </a> </td> <td>Onegai Copilot is an AI coding assistant in JetBrain's IDE. </td> </tr> <tr> <td> <img src="https://github.com/continuedev/continue/blob/main/docs/static/img/logo.png?raw=true" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/continue/README.md"> Continue </a> </td> <td> Continue is an open-source autopilot in IDE. </td> </tr> <tr> <td> <img src="https://raw.githubusercontent.com/a18792721831/studyplugin/535b9cab69da0f97b42dcaebb00bb0d4ed15c8a6/translate/src/main/resources/META-INF/pluginIcon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://plugins.jetbrains.com/plugin/18336-chinese-english-translate">Chinese-English Translate</a> </td> <td> Chinese-English Translate is a multiple translation services in JetBrain's IDE. </td> </tr> <tr> <td> <img src="https://plugins.jetbrains.com/files/24851/659002/icon/pluginIcon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://plugins.jetbrains.com/plugin/24851-ai-git-commit">AI Git Commit</a> </td> <td> This plugin uses AI to automatically generate commit messages based on the changes in your code. </td> </tr> </table> <h3 id="discord-bots">Discord Bots</h3> <table> <tr> <td> <img src="https://geneplore.com/img/geneplore_color_logo_circular.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/Geneplore AI/README.md"> Geneplore AI </a> </td> <td> Geneplore AI runs one of the largest AI Discord bots, now with Deepseek v3 and R1. </td> </tr> </table> <h3 id="native-ai-code-editor">Native AI Code Editor</h3> <table> <tr> <td> <img src="https://global.discourse-cdn.com/flex020/uploads/cursor1/original/2X/a/a4f78589d63edd61a2843306f8e11bad9590f0ca.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://www.cursor.com/"> Cursor </a> </td> <td>‍The AI Code Editor based on VS Code</td> </tr> <tr> <td> <img src="https://exafunction.github.io/public/images/windsurf/windsurf-app-icon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://codeium.com/windsurf"> WindSurf </a> </td> <td>Another AI Code Editor based on VS Code by Codeium</td> </tr> </table> <h3 id="emacs">Emacs</h3> <table> <tr> <td> <img src="https://upload.wikimedia.org/wikipedia/commons/0/08/EmacsIcon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/karthink/gptel"> gptel </a> </td> <td>A simple LLM client for Emacs</td> </tr> <tr> <td> <img src="https://upload.wikimedia.org/wikipedia/commons/0/08/EmacsIcon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/milanglacier/minuet-ai.el"> Minuet AI </a> </td> <td>Dance with Intelligence in Your Code 💃</td> </tr> </table> <h3 id="security">Security</h3> <table> <tr> <td> <img src="https://github.com/lukehinds/awesome-deepseek-integration/blob/codegate/docs/codegate/assets/codegate.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/stacklok/codegate/"> CodeGate </a> </td> <td> CodeGate: secure AI code generation</td> </tr> </table> <h3 id="others">Others</h3> <table> <tr> <td style="font-size: 64px">🤖</td> <td> <a href="https://github.com/wangrongding/wechat-bot/blob/main/README.md"> Wechat-Bot </a></td> <td> A wechat robot based on WeChaty combined with DeepSeek and other Ai services. </td> </tr> <tr> <td style="font-size: 64px">&#128032;</td> <td> <a href="https://github.com/lunary-ai/abso/blob/main/README.md"> Abso </a></td> <td> TypeScript SDK to interact with any LLM provider using the OpenAI format. </td> </tr> <tr> <td> <img src="https://i.imgur.com/IsQYInJ.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/djcopley/ShellOracle/"> ShellOracle </a> </td> <td> A terminal utility for intelligent shell command generation. </td> </tr> <tr> <td> <img src="https://avatars.githubusercontent.com/u/178783630?s=200&amp;v=4" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/bolna-ai/bolna/"> Bolna </a> </td> <td> Use DeepSeek as the LLM for conversational voice AI agents</td> </tr> <tr> <td> <img src="https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/c1e47b01-1766-4f7e-bfe6-ab3cb3991c30" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/tree/main/docs/siri_deepseek_shortcut"> siri_deepseek_shortcut </a> </td> <td> Siri equiped with the DeepSeek API </td> </tr> <tr> <td> <img src="https://github.com/n8n-io/n8n/blob/master/assets/n8n-logo.png?raw=true" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/rubickecho/n8n-deepseek"> n8n-nodes-deepseek </a> </td> <td> An N8N community node that supports direct integration with the DeepSeek API into workflows. </td> </tr> <tr> <td> <img src="https://framerusercontent.com/images/TSKshn2UFdTyvUi85EDMIXrXgs.png?scale-down-to=512" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/Portkey-AI/gateway"> Portkey AI </a> </td> <td> Portkey is a unified API for interacting with over 1600+ LLM models, offering advanced tools for control, visibility, and security in your DeepSeek apps. Python &amp; Node SDK available. </td> </tr> <tr> <td> <img src="https://framerusercontent.com/images/8rF2JOaZ8l9AvM4H6ezliw44aI.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/BerriAI/litellm"> LiteLLM </a> </td> <td> Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format. Supports DeepSeek AI with cost tracking as well. </td> </tr> <tr> <td> <img src="https://i.postimg.cc/k5Z4YWjt/Screenshot-2025-01-23-at-6-08-01-PM.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/mem0ai/mem0"> Mem0 </a> </td> <td> Mem0 enhances AI assistants with an intelligent memory layer, enabling personalized interactions and continuous learning over time. </td> </tr> <tr> <td> <img src="https://www.promptfoo.dev/img/logo-panda.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="docs/promptfoo/README.md"> promptfoo </a> </td> <td> Test and evaluate LLM prompts, including DeepSeek models. Compare different LLM providers, catch regressions, and evaluate responses. </td> </tr> <tr> <td> </td> <td> <a href="https://github.com/AndersonBY/deepseek-tokenizer"> deepseek-tokenizer </a> </td> <td> An efficient and lightweight tokenization library for DeepSeek models, relying solely on the `tokenizers` library without heavy dependencies like `transformers`. </td> </tr> <tr> <td> <img src="https://langfuse.com/icon.svg" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://langfuse.com/docs/integrations/deepseek"> Langfuse </a> </td> <td> Open-source LLM observability platform that helps teams collaboratively debug, analyze, and iterate on their DeepSeek applications. </td> </tr> <tr> <td> CR </td> <td> <a href="https://github.com/hustcer/deepseek-review"> deepseek-review </a> </td> <td> 🚀 Sharpen Your Code, Ship with Confidence – Elevate Your Workflow with Deepseek Code Review 🚀 </td> </tr> <tr> <td> <img src="http://gptlocalhost.com/wp-content/uploads/2025/01/icon_1024.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://youtu.be/T1my2gqi-7Q"> GPTLocalost </a> </td> <td> Use DeepSeek-R1 in Microsoft Word Locally. No inference costs. </td> </tr> <tr> <td> <img src="https://github.com/suqicloud/wp-ai-chat/raw/main/ic_logo.png" alt="Icon" width="64" height="auto"/> </td> <td> <a href="https://github.com/suqicloud/wp-ai-chat"> WordPress ai助手 </a> </td> <td> Docking Deepseek api for WordPress site ai conversation assistant, post generation, post summary plugin. </td> </tr> </table> <h3 id="star-history">Star History</h3> <p><a href="https://star-history.com/#deepseek-ai/awesome-deepseek-integration&amp;Date"><img src="https://api.star-history.com/svg?repos=deepseek-ai/awesome-deepseek-integration&amp;type=Date" alt="Star History Chart"/></a></p>]]></content><author><name></name></author><category term="extracurricular"/><category term="math"/><summary type="html"><![CDATA[Copy from deepseek-ai]]></summary></entry><entry><title type="html">Self-Study Note - Probabilistic Robotics</title><link href="https://jonaruthardt.github.io/blog/2024/pr-1/" rel="alternate" type="text/html" title="Self-Study Note - Probabilistic Robotics"/><published>2024-12-16T15:12:00+00:00</published><updated>2024-12-16T15:12:00+00:00</updated><id>https://jonaruthardt.github.io/blog/2024/pr-1</id><content type="html" xml:base="https://jonaruthardt.github.io/blog/2024/pr-1/"><![CDATA[<blockquote> <p>The following are my self-study notes for the PhD preparatory course, Probabilistic Robotics. This content is intended solely for personal review and to motivate myself. Please email me at <a href="mailto:12011126@mail.sustech.edu.cn">12011126@mail.sustech.edu.cn</a> to report any issue, and the original course metarials are at <a href="https://mitpress.mit.edu/9780262201629/probabilistic-robotics/">https://mitpress.mit.edu/9780262201629/probabilistic-robotics/</a>. As I own the physical book, translated into Simplified Chinese, I would write the following contents in the same language but not English, in order to prevent using wrong academic expressions.</p> </blockquote>]]></content><author><name></name></author><category term="note"/><category term="math"/><summary type="html"><![CDATA[My PhD preparatory course]]></summary></entry><entry><title type="html">Release of a multimodal autonomous driving dataset in a simulation environment</title><link href="https://jonaruthardt.github.io/blog/2024/C4S/" rel="alternate" type="text/html" title="Release of a multimodal autonomous driving dataset in a simulation environment"/><published>2024-09-01T15:12:00+00:00</published><updated>2024-09-01T15:12:00+00:00</updated><id>https://jonaruthardt.github.io/blog/2024/C4S</id><content type="html" xml:base="https://jonaruthardt.github.io/blog/2024/C4S/"><![CDATA[<p>The dataset is free to <strong><a href="https://www.kaggle.com/datasets/ghosnp/carla-4scenes">download</a></strong>, and the collecting tool is <strong><a href="https://github.com/Kazawaryu/CARLA_ADA">open source</a></strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/other/0000017576-480.webp 480w, /assets/img/blog/other/0000017576-800.webp 800w, /assets/img/blog/other/0000017576-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/other/0000017576.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> More details in https://www.kaggle.com/datasets/ghosnp/carla-4scenes </div> <p>The CARLA-ADA Dataset is a comprehensive multimodal autonomous driving dataset collected using our custom data acquisition tool in the CARLA simulator. The dataset encompasses diverse driving scenarios across urban, rural, highway, and suburban environments, providing a rich variety of real-world driving situations.</p> <p>The dataset features synchronized image and 3D LiDAR point cloud data, capturing five distinct object classes: cars, trucks, pedestrians, bicycles, and buses. With a total size of 35GB, this dataset offers high-quality annotated data suitable for various autonomous driving perception tasks, including object detection, semantic segmentation, and multi-modal fusion research.</p> <p>Key Features:</p> <ul> <li>Multi-environment data collection: urban, rural, highway, and suburban scenes</li> <li>Multi-modality: camera images and LiDAR point clouds</li> <li>Five object classes with careful annotations</li> <li>Diverse weather and lighting conditions</li> <li>Comprehensive coverage of real-world driving scenarios</li> </ul> <p>This dataset aims to support research and development in autonomous driving perception systems, particularly for deep learning applications requiring diverse and well-annotated training data.</p>]]></content><author><name></name></author><category term="code"/><category term="AD"/><category term="dataset"/><summary type="html"><![CDATA[Collected by CARLA-ADA from CARLA 0.9.15]]></summary></entry><entry><title type="html">3D Traffic Vulnerable Group Detection in Simulation-driven Autonomous Driving</title><link href="https://jonaruthardt.github.io/blog/2023/Sim2det3D/" rel="alternate" type="text/html" title="3D Traffic Vulnerable Group Detection in Simulation-driven Autonomous Driving"/><published>2023-10-08T15:12:00+00:00</published><updated>2023-10-08T15:12:00+00:00</updated><id>https://jonaruthardt.github.io/blog/2023/Sim2det3D</id><content type="html" xml:base="https://jonaruthardt.github.io/blog/2023/Sim2det3D/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Autonomous driving is one of the most exciting topics in the fields of machine learning and deep learning. In recent years, the technology behind autonomous driving has advanced rapidly in both academia and industry. The various aspects of autonomous driving can be divided into three main modules: perception, prediction, and decision-making. Due to the data-driven nature of deep learning models, effective algorithms require high-quality data sets. If these high standards are not met, the desired outcomes are unlikely to be achieved. Currently, it is acknowledged that in the decision-making module, training on 1 million kilometers of data can lead to better results. However, no similar benchmark exists for the perception module.</p> <p>Previous 3D Lidar detection algorithms often overlooked vulnerable traffic groups, such as pedestrians and cyclists. We aim to propose a straightforward strategy or framework that maximizes the utility of unit data frames within enhanced datasets, thereby improving the algorithm’s detection of vulnerable traffic groups.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/framework-480.webp 480w, /assets/img/blog/sim2det/framework-800.webp 800w, /assets/img/blog/sim2det/framework-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/framework.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The simple main pipeline of this project </div> <p>Unfortunately, Lidar sensors are expensive, making them unaffordable for our project team. Furthermore, collecting and annotating Lidar data poses significant challenges. Compared to traditional data or image data for autonomous driving, Lidar data is often “too sparse, abstract in description, and difficult to visualize.” This is especially true for identifying traffic-disadvantaged groups, as labeling them based on real data is particularly challenging.</p> <p>To address these issues, our project employs a simulation environment for data collection and preparation. We have developed a set of tools to automatically collect and annotate data within this simulation environment, which allows us to directly extract the 3D positions of targets for annotation. Additionally, given the low frequency of vulnerable groups in existing datasets, we constructed simple scenarios in high dimensions to increase the sample size of these groups.</p> <p>In summary, to tackle the challenges posed by the small and unclear samples of vulnerable traffic groups (pedestrians and cyclists) in traditional datasets, our project has built custom collection tools based on a simulation environment. We also proposed a strategic framework to enhance data collection. To verify its effectiveness, we tested our custom dataset across multiple algorithms, observing a significant improvement in the detection of vulnerable traffic groups.</p> <h1 id="related-work">Related Work</h1> <p>CARLA is a powerful open-source simulator designed for autonomous driving research. It can create a virtual urban environment and simulate various sensors, including cameras, LiDAR, and mmWave radar, to provide essential data. Many researchers have developed their self-driving systems within the Carla environment. By utilizing established object detection algorithms such as YOLO and Faster R-CNN to process the data generated by Carla, they can effectively implement object detection in their systems. The same applies to object tracking, where algorithms like GOTURN and Deep SORT can be employed to achieve successful tracking.</p> <p>For this project, we will use the open-source 3D point cloud detection algorithm training framework, OpenPCDet. This framework is currently a popular and lightweight option for point cloud algorithm training and supports a variety of network architectures.</p> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Car@R11</strong></th> <th><strong>Pedestrian@R11</strong></th> <th><strong>Cyclist@R11</strong></th> <th><strong>Dataset</strong></th> </tr> </thead> <tbody> <tr> <td>PointPillar</td> <td>77.28</td> <td>52.29</td> <td>62.68</td> <td>KITTI</td> </tr> <tr> <td>SECOND</td> <td>78.62</td> <td>52.98</td> <td>67.15</td> <td>KITTI</td> </tr> <tr> <td>Voxel R-CNN</td> <td>84.54</td> <td>-</td> <td>-</td> <td>KITTI</td> </tr> <tr> <td>BEVFusion</td> <td>67.75</td> <td>-</td> <td>-</td> <td>nuScenes</td> </tr> <tr> <td>CenterPoint</td> <td>78.08</td> <td>49.74</td> <td>67.22</td> <td>ONCE</td> </tr> <tr> <td>Voxel NeXt</td> <td>30.05</td> <td>-</td> <td>-</td> <td>Argoverse2</td> </tr> </tbody> </table> <p>Among these, we chose the PointPillar model for experiments to verify that our method has improved the detection effect of if groups. Voxel R-CNN was selected for experiments to verify that the traditional vehicle detection effect has also been improved.</p> <h1 id="methodology">Methodology</h1> <h2 id="co-simulation">Co-simulation</h2> <p>This section primarily focuses on creating the simulation scene. In the Carla map, there are numerous static vehicles that are integral to the map design and are not generated through program control. Consequently, their 3D positions do not appear in memory and cannot be annotated. During the training and verification phases, the model may detect these static vehicles but might misclassify them due to the lack of annotations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/ue41-480.webp 480w, /assets/img/blog/sim2det/ue41-800.webp 800w, /assets/img/blog/sim2det/ue41-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/ue41.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/ue42-480.webp 480w, /assets/img/blog/sim2det/ue42-800.webp 800w, /assets/img/blog/sim2det/ue42-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/ue42.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Operate source code version Carla in the Unreal4 </div> <p>Therefore, we first operate in the source version of Carla and use the Unreal4 toolkit to eliminate static vehicles. After that, we used the Carla-Apollo Bridge to let Apollo take over Carla’s dynamic scene settings and perform visual operations in DreamViewer.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/cosim1-480.webp 480w, /assets/img/blog/sim2det/cosim1-800.webp 800w, /assets/img/blog/sim2det/cosim1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/cosim1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/cosim2-480.webp 480w, /assets/img/blog/sim2det/cosim2-800.webp 800w, /assets/img/blog/sim2det/cosim2-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/cosim2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Operate source code version Carla in the Unreal4 </div> <h2 id="problem-definition">Problem Definition</h2> <p>Given a 3D point cloud, \(\mathbf{P}=\{p_1,p_2,...,p_n\}\), which represents the set of measured points, \(p_i=(x_i,y_i,z_i)\), and presents a snapshots of the surroundings. For objects, \(\mathbf{O}=\{o_1,o_2,...,o_m\}\), represents the set of all objects in point cloud (vehicle, traffic light, pedestrian, etc.), using KITTI format, where \(o_j=(x_j,y_j,z_j,w_j,h_j,l_j,r_j)\).</p> <p>Use \(S_{keep}\) to measure whether to keep the Lidar data.</p> <p>\(S_{keep} \equiv \mathcal{E} (P) \cdot \mathcal{P}(\rho(O),\tau(O))\) where $\mathcal{E}$ decides whether to keep the entire point cloud, and $\mathcal{P}$ decides whether to keep the collected target data.</p> <p>Use \(S_{value}\) to represent the value of current Lidar data for the trainning model,</p> \[S_{value}\equiv \phi(P, O) \cdot \psi(O)\] <p>where \(\phi(P, O)\) is the perception distance term, used to describe the relationship between the effective perception radius and the farthest perception radius; \(\psi(O)\) is the prediction accuracy term, used to describe the perception How accurate is the prediction of objects within the radius.</p> <h2 id="pedestrian-control-algorithm">Pedestrian Control Algorithm</h2> <p>For pedestrian control in the Co-Simulate link, there is the following algorithm. It controls pedestrian behavior during the time between two timestamps, where $pos$ is the position, $range$ is perception range (only 160 degrees in front of the eyes), $towards$ is the absolute angle of orientation, and $speed$ is the speed of a pedestrian, respectively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/code-480.webp 480w, /assets/img/blog/sim2det/code-800.webp 800w, /assets/img/blog/sim2det/code-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/code.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Simple Pedestrian control pesudo code </div> <h2 id="feature-upgrade">Feature Upgrade</h2> <p>The following is the entire process of our improved VFE processing stage.</p> <ul> <li>De-mean encoding \((P, M, 4)\xrightarrow{}(P, M, 3)\) for each point in Pillar</li> <li>Decentralize the effective points in each Pillar \((P, M, 4)\xrightarrow{} (P, M, 2)\)</li> <li>Mask merge coding: Combine the original \((P, M, 4)\) with the above two codes cat to get the vector of \((P, M, 9)\). There are two points to note here: <ul> <li>Only valid points (\(n\) points per pillar) are operated in each Pillar. If the number of valid points is insufficient, zero will be added, if there are too many, random sample will be used;</li> <li>In the code, the 9-dimensional encoding vector is The first 2 dimensions are replaced by decentralized encoded vectors</li> </ul> </li> <li>Convolution kernel pooling: \((P, M, 9)\xrightarrow{}(P, M, 64)\) and \((P, 64)\)</li> <li>Pillarscatter: Go to the 2D feature map of \((X/vsize, Y/vsize)\) and get the feature map of \((64, X/vsize, Y/vsize)\).</li> </ul> <p>The PointPillar model utilizes a method that differs from Voxel in describing point clouds by employing Pillars, which disregards certain information along the Z-axis. During the VFE encoding process, since the model does not take this information into account, it can also omit it during encoding. This approach enhances coding speed and reduces both training and inference times.</p> <h1 id="experiment">Experiment</h1> <p>A total of three data sets were collected, of which A did not use the scenes we built, and B and D used the scenes we built. Each data set consists of 5 subsets, and the number of “vehicles (including cyclists) and pedestrians” in each subset are \((50,25)\), \((75,37)\), \((100,50)\), \((125,62)\) and \((150,75)\).</p> <table> <thead> <tr> <th><strong>Dataset name</strong></th> <th><strong>Total Frames</strong></th> <th><strong>Map</strong></th> <th><strong>Detail</strong></th> </tr> </thead> <tbody> <tr> <td>A</td> <td>987</td> <td>Town05</td> <td>City</td> </tr> <tr> <td>B</td> <td>902</td> <td>Town02</td> <td>Town</td> </tr> <tr> <td>D</td> <td>988</td> <td>Town06</td> <td>Highway</td> </tr> <tr> <td>V</td> <td>375</td> <td>-</td> <td>Random select from A,B, D</td> </tr> </tbody> </table> <p>Then, use datasets A, B, and D to train on PointPillar and Voxel R-CNN, respectively. Use epoch = 160, batch size = 18, dynamically adjust the learning rate, and set Random seed = 114. This results in a total of 6 models. All model training is performed on the server, and the server parameters are as follows.</p> <ul> <li>CPU: Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz</li> <li>GPU: NVIDIA TITAN V \(\times \ 6\)</li> <li>OS: Ubuntu 22.04.2 LTS</li> <li>MEM: 453 G</li> </ul> <h1 id="results">Results</h1> <p>Via TensorBoard tool, the following is the exported curve of the loss decreasing as the step increases.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/loss_pp-480.webp 480w, /assets/img/blog/sim2det/loss_pp-800.webp 800w, /assets/img/blog/sim2det/loss_pp-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/loss_pp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/loss_vr-480.webp 480w, /assets/img/blog/sim2det/loss_vr-800.webp 800w, /assets/img/blog/sim2det/loss_vr-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/loss_vr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Training loss curves of Pointpillars (left) and Voxel R-CNN (right). </div> <p>The following are the evaluation results of Pointpillars after training in OpenPCDet. The indicator uses mAP70, that is, mAP above 70 is calculated as correct recognition.</p> <table> <thead> <tr> <th><strong>Dataset</strong></th> <th><strong>Car</strong></th> <th><strong>Truck</strong></th> <th><strong>Van</strong></th> <th><strong>Pedestrian</strong></th> <th><strong>Cyclist</strong></th> </tr> </thead> <tbody> <tr> <td>A</td> <td>53.86</td> <td>68.32</td> <td>52.78</td> <td>38.08</td> <td>45.11</td> </tr> <tr> <td>B</td> <td>60.28</td> <td>71.27</td> <td>54.11</td> <td>40.87</td> <td><strong>56.30</strong></td> </tr> <tr> <td>D</td> <td>64.09</td> <td>68.59</td> <td>71.09</td> <td><strong>48.05</strong></td> <td>52.26</td> </tr> </tbody> </table> <p>Similarly, the result of Voxel-RCNN is shown as the followed.</p> <table> <thead> <tr> <th><strong>Dataset</strong></th> <th><strong>Vehicle</strong></th> </tr> </thead> <tbody> <tr> <td>A</td> <td>8.63</td> </tr> <tr> <td>B</td> <td><strong>64.28</strong></td> </tr> <tr> <td>D</td> <td>63.21</td> </tr> </tbody> </table> <p>Visual display of some data of the model on the test set.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pd1-480.webp 480w, /assets/img/blog/sim2det/pd1-800.webp 800w, /assets/img/blog/sim2det/pd1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pd1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pd1-1-480.webp 480w, /assets/img/blog/sim2det/pd1-1-800.webp 800w, /assets/img/blog/sim2det/pd1-1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pd1-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pd2-480.webp 480w, /assets/img/blog/sim2det/pd2-800.webp 800w, /assets/img/blog/sim2det/pd2-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pd2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pd2-1-480.webp 480w, /assets/img/blog/sim2det/pd2-1-800.webp 800w, /assets/img/blog/sim2det/pd2-1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pd2-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pv1-480.webp 480w, /assets/img/blog/sim2det/pv1-800.webp 800w, /assets/img/blog/sim2det/pv1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pv1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pv1-1-480.webp 480w, /assets/img/blog/sim2det/pv1-1-800.webp 800w, /assets/img/blog/sim2det/pv1-1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pv1-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualization of different models trained on different data sets under the numbered data frames shown </div> <p>It is not difficult to find that the PointPillar model can basically accurately detect small objects in the distance. Voxel R-CNN has correctly and completely detected all vehicles in this scene, even those with severe occlusion.</p> <h1 id="conclusions">Conclusions</h1> <p>In this prpject, we address the current oversight of vulnerable traffic groups, such as pedestrians and cyclists, in 3D detection algorithms. We propose a project framework utilizing the Carla simulation environment, which encompasses scene construction, data preparation, and model training. We conducted experiments using the PointPillar algorithm and the Voxel R-CNN algorithm within the OpenPCDet framework. The experimental results demonstrate that our approach is more effective, significantly enhancing the 3D detection capabilities for vulnerable traffic groups while also improving performance in general vehicle detection tasks.</p>]]></content><author><name></name></author><category term="course-work"/><category term="AD"/><summary type="html"><![CDATA[Introduces a new joint simulation strategy that combines Carla and Apollo to enhance data quality and improve model performance.]]></summary></entry><entry><title type="html">A Review of 3D Point Cloud Detection Algorithms - from PointNet to VoxelNeXt</title><link href="https://jonaruthardt.github.io/blog/2023/pcd-survey/" rel="alternate" type="text/html" title="A Review of 3D Point Cloud Detection Algorithms - from PointNet to VoxelNeXt"/><published>2023-08-17T15:12:00+00:00</published><updated>2023-08-17T15:12:00+00:00</updated><id>https://jonaruthardt.github.io/blog/2023/pcd-survey</id><content type="html" xml:base="https://jonaruthardt.github.io/blog/2023/pcd-survey/"><![CDATA[<blockquote> <p>This article introduces some common 3D point cloud detection algorithms (as of August 2023). The content comes from personal understanding and may have some problems.</p> </blockquote> <h1 id="1-摘要">1. 摘要</h1> <p>近年来，激光雷达点云目标检测算法在自动驾驶、机器人感知等领域中扮演着重要角色。本综述旨在回顾激光雷达点云目标检测算法的发展历程，重点关注从经典的PointNet算法到最新的VoxelNext算法的演进过程。我将从算法的基本原理、方法优劣势、实验结果等方面综合评述各个算法，并提出当前研究的挑战和未来发展方向。作为激光雷达点云目标检测算法的学习启蒙。</p> <h1 id="2-引言">2. 引言</h1> <h2 id="21-背景">2.1 背景</h2> <p>自动驾驶和机器人技术的快速发展推动了激光雷达点云目标检测算法的研究。激光雷达点云作为一种重要的三维感知数据，具有高精度和丰富的空间信息，因此被广泛应用于目标检测任务中。感知环节，又可以细分为多个模块。具体来讲，从 Detection, Postion, On-board Map 三个方面考虑。本文主要考虑的是其中的 Detection 方面。该环节主要考虑如何实时检测障碍物、目标物等。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/%E5%BC%95%E8%A8%80-480.webp 480w, /assets/img/blog/pcd-survey/%E5%BC%95%E8%A8%80-800.webp 800w, /assets/img/blog/pcd-survey/%E5%BC%95%E8%A8%80-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/%E5%BC%95%E8%A8%80.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 3D激光雷达点云目标检测 </div> <ul> <li>2016年，PointNet首次提出点云分割的基本思路、骨干网络，以严谨而全面的实验验证算法各个环节的有效性。</li> <li>2017年，VoxelNet以 PointNet 为基础，用 Voxel 描述点云空间，提出VFE（Voxel Feature Encoder，体素特征提取器），作为往后几乎一切点云目标检测算法的首要环节。</li> <li>2018年，SECOND使用改进的稀疏卷积和新的损失函数，对 VoxelNet 进行了一些优化和完善，采用改良的稀疏卷积，效果改善显著。</li> <li>2019年，PointPillar 将VoxelNet中Voxel改为长条形的Pillar，采用SECOND中的稀疏卷积核，几乎不影响性能的情况下极大地提升了性能，是工业界常用的算法之一。</li> <li>2020年，Voxel R-CNN基于voxel特征，设计了3D点云目标检测的two stage网络，提升了些许检测精度。</li> <li>2021年，CenterPoint继承 PointPillar 算法的Pillar化思路，并结合2D的CenterNet算法，引入注意力机制，看到了在3D检测上采取2D检测策略的优势性，进一步提升two stage系列算法性能。</li> <li>2023年，Voxel NeXt回归到点云数据本质，采用纯稀疏的卷积网络，解决了CenterPoint中注意力浪费的部分，并结合Segment Anything开发出一套强鲁棒检测算法驱动的自动标注系统。</li> </ul> <p>以上是近年来一些代表性比较强的点云目标检测算法的发展历程。本文将对其中PointNet，VoxelNet，CenterPoint 以及 VoxelNeXt四个算法进行详细的介绍，并分析算法的创新点，以及一些思考。</p> <h2 id="22-问题定义">2.2 问题定义</h2> <p>3D目标检测是自动驾驶的一个基础部分，3D目标检测网络通常输入稀疏点云，输出物体三维位置以及类别。硬件设备常常选用激光雷达Lidar。</p> <p>对于整个点云数据帧$F$，其内部有点集\(P = \{p_1,p_2,...,p_n\}, p_k=\{x_k,y_k,z_k,I_k\} , p_k \in P\)（\(I\)表示点的强度信息）。对\(P\)执行检测算法\(A\)，返回预测目标集\(D=\{d_1,d_2,...,d_m\}, d_i = \{x_i,y_i,z_i,h,w,l,rot,lab\}, d_i\in D\)，其中\(x,y,z\)表示3D框的中心点，\(h,w,l\)表示3D框的高、宽、长，\(rot\)表示在6轴坐标系下的\(yaw\)旋转角，\(lab\)表示3D框的语义信息，即预测目标的类别。</p> <h2 id="23-评估标准">2.3 评估标准</h2> <h3 id="231-metrics-based-on-average-precision">2.3.1 Metrics based on Average Precision</h3> <p>检测算法的评估，一般采用 IoU (Inner of Union) 作为评价 BBox 和 Ground-Truth 的重合程度。数学建模为，对于同一个对象\(O\)，取算法预测框$D$与真值框\(G\) (Ground Truth) ，按如下公式计算：</p> \[IoU = \frac{area(D)\cap area(G)}{area(D)\cup area(G)}\] <p>定义 \(IoU&gt;0.5\)为真阳性 \(TP\) (True Positive)，\(IoU\le0.5\)为假阳性\(FP\)（False Positive）；定义真值未检测的为假阴性$FN$(False Negivate)，定义检测出但真值不存在的为真阴性\(TN\)(True Positive)。根据以上几个定义，可以计算出模型整体的准确度$P$(Precision) 与召回率\(R\)(Recall)，并绘制成 \(P-R\) 曲线。</p> \[P=\frac{TP}{FP+FN}\ \ \ R=\frac{TP}{TP+FN}\] <p>计算曲线的下积分面积\(AP\)(Average Precision)。近似地，用以下公式估计\(AP\)的值，表示为对于此次测试的数据，模型整体的平均准确度。对所有测试数据求平均，求的\(mAP\)用于评估整个模型在测试集上的效果。</p> <p>\(AP = \sum_{k=0}^{k = n - 1}[Recall(k) - Recall(k-1)] \times Precision(k)\) \(mAP = \frac{1}{n} \sum_{k = 1}^{k = n}AP_k\)</p> <p>进一步地，更改\(IoU\)的阈值，从 \(0.5\)以\(0.05\)为单位递增至\(0.95\)，计算每个阈值下的\(mAP\)，更严格地评价模型效果，将该指标称为$mAP50-95$。 对于点云数据，由于其数据特有的稀疏性，通过考虑地平数据点比较稀疏和离散，不好作特征提取，由面上的 2D 中心距离而不是基于联合的亲和力的交集来定义匹配。具体来说，将预测与具有最小中心距离且达到特定阈值的地面真实对象进行匹配。对于给定的匹配阈值，我们通过整合召回率与精度曲线（召回率和精度 &gt; 0.1）来计算$AP$。我们最终对 \(\{0.5,1,2,4\}\) 米的匹配阈值进行平均，并计算各个类别的平均值。</p> <h3 id="232-metrics-based-on-true-positive">2.3.2 Metrics based on True Positive</h3> <p>依据nuScenes提出的标准，定义一组真阳性 (TP) 的指标，用于测量平移、尺度、方向、速度和属性错误。所有\(TP\)指标都是在匹配时使用$2m$中心距的阈值计算的，并且它们都被设计为正标量。</p> <p>每个类别的匹配和评分都是独立进行的，每个指标都是每个达到 10% 以上的召回水平的累积平均值的平均值。如果特定类别未达到 10\% 的召回率，则该类别的所有 \(TP\) 错误都将设置为 1。我们定义以下 \(TP\) 错误：</p> <ul> <li>平均平移误差 ATE (Average Translation Error)：二维欧几里得中心距离（以米为单位）；</li> <li>平均尺度误差 ASE (Average Scale Error)：在对齐中心和方向后计算为\(1 - IOU\) ；</li> <li>平均方向误差 AOE(Average Orientation Error)：预测与地面实况之间的最小偏航角差异（以弧度为单位）。对于所有类别，方向误差均以 360 度进行评估，障碍除外，障碍物仅以 \(180\)度进行评估。忽略锥体的方向错误；</li> <li>平均速度误差 AVE (Average Velocity Error)：绝对速度误差，以 \(m/s\) 为单位。障碍物和锥体的速度误差被忽略；</li> <li>平均属性误差 AAE (Average Attribute Error)：计算为\(1 - acc\)，其中 $acc$ 是属性分类精度。障碍物和锥体的属性错误被忽略。</li> </ul> <p>以上5个标准与mAP加权求和，即nuScenes定义的NDS (nuScenes Detection Score) 标准，即</p> \[\text{NDS} = 1/10[5\text{mAP}+ {\sum}_{\text{mTP}\in\mathbb{TP}} (1-\min(1, \text{mTP}))]\] <p>以上所有误差都 &gt;= 0，但是，对于平移误差和速度误差，误差是无界的，并且可以是任何正值。此类 $TP$ 指标是按类定义的，然后我们取类的平均值来计算 \(mATE, mASE, mAOE, mAVE,mAAE\)。</p> <h2 id="24-任务难点">2.4 任务难点</h2> <p>由于深度学习数据驱动的特性，就算法训练而言，在数据获取上，相比与其他数据，3D点云数据集的制备和处理非常困难。</p> <p>对比于 2D 图像数据，处理点云数据具有以下几个难点。在后文中，会分别简要提到如何克服以下几个困难。数据点比较稀疏和离散，不好作特征提取，噪声不好区别；具有更多自由度，不好处理（更多的尺寸种类，相对于坐标周有多个维度的旋转）；数据标注困难，每个Ground Truth（真值框）需要八个点位的标注。</p> <h2 id="25-常见的公开数据集">2.5 常见的公开数据集</h2> <p>针对数据获取的困难，往往采用一些常见的开源点云数据集进行训练。</p> <ul> <li>KITTI 3D：具备1.5w帧数据，8w个标注的目标；</li> <li>nuScenes：具备39w帧雷达，140w个标注目标，23类；</li> <li>Waymo：具备200w帧，2260w个标注目标，4类+23类；</li> <li>Argoverse2：具备1000个场景，每帧图像平均有75个目标物，30个类别；</li> <li>Dense: 不同天气下12000个样本和浓雾中的1500个样本，少见的极端天气数据集；</li> <li>SUScape：南方科技大学与Intel公司合作，在深圳采集的数据集，具有40+类别，有效平均实例密度为nuScene的10倍。</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/nuScene%E8%BD%A6-480.webp 480w, /assets/img/blog/pcd-survey/nuScene%E8%BD%A6-800.webp 800w, /assets/img/blog/pcd-survey/nuScene%E8%BD%A6-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/nuScene%E8%BD%A6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> nuScenes数据集车辆传感器设置 </div> <p>如果想要自己制备数据集用来训练模型，往往采用以下的方法。</p> <ul> <li>自制真实数据集：机器人采集真实数据，通过 Segement Anything，SUSTechPoint 等工具辅助标注；</li> <li>仿真采集数据集：Carla 等仿真环境，数据标注来源于对仿真环境真值的处理。目前有一种主动式的方针采集思路CARLA-ADA，能够制备比较有效的仿真数据集。</li> </ul> <h1 id="3-算法介绍">3. 算法介绍</h1> <h2 id="31-pointnet-2016">3.1 PointNet-2016</h2> <p>PointNet 和 PointNet++ 最早被提出的一类3D点云分割模型 (Lidar Segmentation)，作为点云目标检测的先驱和奠基。</p> <h3 id="311-算法摘要">3.1.1 算法摘要</h3> <p>由斯坦福大学于 2016 年发表论文 “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation”，是点云神经网络的鼻祖，它提出了一种网络结构，可以直接从点云中学习特征。</p> <p>该文章在分类、语义分割两种任务上做出了对比，并给了理论和实验分析。点云的特点其实非常好理解，只要网络抓住以下三个特点，那么它至少就能作为一个能用的 encoder 。</p> <ol> <li>排列不变性：重排一遍所有点的输入顺序，所表示的还是同一个点云数据，网络的输出应该相同。</li> <li>点集之间的交互性：点与点之间有未知的关联性。</li> <li>变换不变性：对于某些变换，例如仿射变换，应用在点云上时，不应该改变网络对点云的理解。</li> </ol> <h3 id="312-创新点">3.1.2 创新点</h3> <p>基于以上提到的点云数据三个特点，PointNet 对问题的处理是：</p> <ol> <li>排列不变性：该文章使用了对称函数（Symmetry Function），它的特点是对输入集合的顺序不敏感。这种函数非常多，如 maxpooling，加法，向量点积等。PointNet 采用的是 maxpooling 方法来聚合点集信息。</li> <li>点集之间的交互性：实际上，对称函数的聚合操作就已经得到了全局的信息，此时将点的特征向量与全局的特征向量 concat 起来，就可以让每个点感知到全局的语义信息了，即所谓的交互性。</li> <li>变换不变性：只需要对输入做一个标准化操作即可。PointNet 使用网络训练出了 D 维空间的变换矩阵。</li> </ol> <h3 id="313-网络结构">3.1.3 网络结构</h3> <p>PointNet 网络分为两个部分：分类网络 (Classifiction Network) 和分割网络 (Segmentation Network)。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/pointnet-structure-480.webp 480w, /assets/img/blog/pcd-survey/pointnet-structure-800.webp 800w, /assets/img/blog/pcd-survey/pointnet-structure-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/pointnet-structure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PointNet算法网络结构 </div> <h4 id="分类网络">分类网络</h4> <p>基本思路：分类网络以 \(n\) 个点作为输入，对输入做特征变换（变换矩阵），将结果输入给 MLP 做回归。将上一步的结果放在特征空间中做特征变换，再输入给 MLP 回归，对得到的输出进行 Maxpool 挑选最大值，作为整个 3D 点云的全局特征。 分类网络设计 Gloabl Feature，这一步称为 Symmetry Function for Unordered Input，对输入做处理。具体来讲，用一个简单的对称函数聚集每个点的信息：</p> \[f(\{x_1,...,x_2\}) \approx g(h(x_1),...,h(x_n))\] <p>对此过程数学建模，\(f\)为目标，$g$ 为设计的对称函数。从公式来看，其基本思路是：对各个点\(x_k\)分别做$h$处理，再将所有处理后的点交由函数$g$处理，以实现排列不变性。在实现中，\(h\)为 MLP，\(g\)为 maxpooling。</p> <h4 id="分割网络">分割网络</h4> <p>基本思路：分割网络将经过特征空间变换后的点局部特征 (local) 与全局特征 (global) 拼接，输入给 MLP 处理，对每一个点进行分类。</p> <p>分割网络获取 Point-wise Feature，这一步称为 Local and Global Information Aggregation，对两个不同维度的特征做拼接。</p> <p>但由于特征空间中的变换矩阵维度远远大于空间中的变换矩阵维度，在softmax训来时，用一个正则化项，将特这个变换矩阵限制为近似的正交矩阵，即输出尺度归一化。这一步称为 Joint Alignment Network：</p> \[L_{reg}={||I-AA^T||}^2_F\] <p>其中\(A\)是维度较小网络预测的特征对齐矩阵。消融实验证明该步骤可以有效优化，使模型效果更稳定。</p> <h3 id="314-模型效果">3.1.4 模型效果</h3> <p>数据援引论文原文，指标为点的 mIoU(\%)。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/pointnet%E6%95%88%E6%9E%9C-480.webp 480w, /assets/img/blog/pcd-survey/pointnet%E6%95%88%E6%9E%9C-800.webp 800w, /assets/img/blog/pcd-survey/pointnet%E6%95%88%E6%9E%9C-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/pointnet%E6%95%88%E6%9E%9C.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PointNet算法效果 </div> <h3 id="315-算法总结">3.1.5 算法总结</h3> <p>PointNet之所以影响力巨大，并不仅仅是因为它是第一篇点云目标检测文章，更重要的是它的网络很简洁（简洁中蕴含了大量的工作来探寻出简洁这条路）却非常的work，这也就使得它能够成为一个工具，一个为点云表征的encoder工具，应用到更广阔的点云处理任务中。</p> <p>仅用 MLP+max pooling 就击败了众多SOTA，令人惊讶。另外PointNet在众多细节设计也都进行了理论分析和消融实验验证，保证了严谨性，这也为PointNet后面能够大规模被应用提供了支持。</p> <p>由于 PointNet 模型只使用了 MLP 和 Maxpooling，所获得的特征是全局的，没有捕获局部结构特在，在细节处理和泛用性都不是很好。为使得特征更关注于“局部”，对 3D 点云进行有重叠的多次降采样，分别对每次采样做特征提取，最后进行拼接，其余思路和 PointNet 类似。</p> <h2 id="32-voxelnet-2017">3.2 VoxelNet-2017</h2> <h3 id="321-算法摘要">3.2.1 算法摘要</h3> <p>由 Apple 公司于 2017 年发表论文 “VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection”，是 3D 点云目标检测真正利用好体素化 (Voxel) 的第一篇文章。</p> <p>在此之前，3D 点云目标检测的主流方法是：(1).数据降维：3D 数据投影成 2D 图像，用传统目标检测方法计算 (2). 整个点云 Voxel 分割后手工设计特征。这些方法对点云数据的利用不足。</p> <p>VoxelNet 属于单阶段，端到端的点云检测。大致流程为：按空间位置划分 Voxel，然后将 Voxel 内部的点云进行 VFE (Voxl feature encoding) 编码，再接入RPN来生成检测结果，在KITTI数据集上表现出色。</p> <h3 id="322-创新点">3.2.2 创新点</h3> <p>对于 VoxelNet 模型，其突破点和创新点是用 Voxel 描述空间，使用 PointNet 网络对 Voxel 进行特征提取，用该特征代表每个 Voxel ，并放回 3D 空间中，使点云数据有序化。完成三维的空间的特征描述后，对 Voxel 做三维卷积，在得到的特征图上进行目标检测。其实现方式可以说成是，对每个voxel使用PointNet得到voxel的feature。</p> <h3 id="323-网络结构">3.2.3 网络结构</h3> <p>VoxelNet 网络分为三个层级：特征学习网络 (Feature Learning Network)、卷积中间层 (Convolutional MIddle Layers)、区域候选网络 (Region Proposal Network)。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/voxelnet%E7%BB%93%E6%9E%84-480.webp 480w, /assets/img/blog/pcd-survey/voxelnet%E7%BB%93%E6%9E%84-800.webp 800w, /assets/img/blog/pcd-survey/voxelnet%E7%BB%93%E6%9E%84-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/voxelnet%E7%BB%93%E6%9E%84.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> VoxelNet算法网络结构 </div> <h4 id="特征学习网络">特征学习网络</h4> <p>通过以下五个步骤，提取和学习 3D 点云数据特征。Voxel 化点云、聚合和随机采样很好地解决了“数据点比较稀疏和离散，不好作特征提取”的问题。</p> <ol> <li>Voxel 划分：在雷达坐标系中，对于空间 \(D\{X,Y,Z\}\)（针对要检测的物体，会切割出不同的长方体），按\(V\{v_x,v_y,v_z\}\)为单位，划分为小的Voxel，有\(Z'=Z/v_z, Y'=Y/v_y, X'=X/v_x\)。</li> <li>聚合：由于雷达数据的离散性，点云在三维空间内的分布不均匀，也有相当可能出现空检，将相邻的Voxel进行聚合，尽量减少这种对网络训练不利的情况出现。</li> <li>随机采样：对于聚合后的Voxel，随机在非空的Voxel内采样\(T\)个点。这一步后，将点云数据表示为\(\{N,T,C\}\)，\(N\)为非空Voxel个数，\(T\)为每个Voxel内的随机采样点个数 ，\(C\)为点的特征。对于不足\(T\)个点的 Voxel，采用 “高斯补0” 算法。</li> <li>VFE（Voxel Feature Enocding）堆叠： <ol> <li>对于Voxel，首先数据增强其中的每一个点，计算平均值，再计算每个点的偏移量，与原始数据拼接作为输入（Point-wise Input）；</li> <li>采用PointNet模型的方法，将Voxel中的点通过全连接层（FCN）转化到特征空间 (Point-wise Feature)，在新的特征中挑选出特征值最大的点 (Element-wise Maxpool)，作为每个Voxel的表面形状特征 (Locally Aggregated Feature)； (c). 获取每个Voxel的特征后，最后再拼接 (Point-wise Concatenate)成为更高维的特征 (Point-wise concatenated Feature)。</li> </ol> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/VFE-480.webp 480w, /assets/img/blog/pcd-survey/VFE-800.webp 800w, /assets/img/blog/pcd-survey/VFE-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/VFE.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> VFE：体素特征编码器的编码过程 </div> <p>特征提取后稀疏特征的表示：上一步中，都是对非空的 voxel 进行处理，这些 voxel 仅仅对应 3D 空间中很小的一部分空间。这里需要将得到的 N 个非空的 voxel 特征重新映射回来源的3D空间中，表示成一个稀疏的 4D张量，\(（C，Z'，Y'，X'）-&gt; (128, 10, 400, 352)\)。这种稀疏的表示方法极大的减少了内存消耗和反向传播中的计算消耗。同时也是 VoxelNet 为了效率而实现的重要步骤。</p> <h4 id="卷积中间层">卷积中间层</h4> <p>简单来讲，用以下三个三维卷积核对 Voxel 化的点云进行卷积，每个卷积后都接 BN 层 (Batch Normalization) 和 ReLU 层，增强传递防止梯度消失，归一化加速网络收敛。通过卷积中间层，能够提升感受视野</p> <p>\(Conv3D_1(128, 64, 3, (2,1,1), (1,1,1))\) \(Conv3D_2(64, 64, 3, (1,1,1), (0,1,1))，\) \(Conv3D_3(64, 64, 3, (2,1,1), (1,1,1))\)</p> <p>通过中间层运算之后，4D的tensor尺寸为（拿car detection为例，62<em>2</em>400<em>352），然后会进行reshape操作，将特征图变成 128</em>400*352便于后续使用RPN。</p> <h4 id="区域候选层">区域候选层</h4> <p>RPN层的概念在FasterRCNN中就被提出来，主要是用于根据特征图中学习到的特征和结合anchor来生成对应的预测结果。VoxelNet的预测头，类似于SSD和YOLO那一类的目标检测算法种的头预测。在 FrCNN 中 RPN 在每个像素和像素的中心点位置根据 anchor 的设置，预测了一个 anchor 属于类别，以及针对该 anchor 的粗回归调整。在VoxelNet 中也不例外。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/RPN-480.webp 480w, /assets/img/blog/pcd-survey/RPN-800.webp 800w, /assets/img/blog/pcd-survey/RPN-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/RPN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> RPN：区域候选网络的特征堆叠过程 </div> <p>VoxelNet 的 RPN 结构在经过前面的 Convolutional middle layers 和 tensor 重组得到的特征图后，对这个特征图分别的进行了多次下采样，然后再将不同下采样的特征进行反卷积操作，变成相同大小的特征图。再拼接这些来自不同尺度的特征图，用于最后的检测。给人的感觉类似图像目标检测的 NECK 模块中 PAN，只不过这里只有一张特征图。将不同尺度的信息融合在了一起。这里每一层卷积都是二维的卷积操作，每个卷积后面都接一个BN和RELU层。输出的结果是一个分类预测和 anchor 回归预测的结果。</p> <h3 id="324-锚点选择与损失函数">3.2.4 锚点选择与损失函数</h3> <p>锚点的选择采用（拿car detection举例）选取 \(l^a=3.9,w^a=1.6,h^a=1.59,z^a=-1.0,\theta=[0,90]\)。定义\(\{a_i^{pos}\}_{i=1...N_{pos}}\)为正样本锚点，\(\{a_i^{neg}\}_{i=1...N_{neg}}\)为负样本锚点。检测框被参数化成 \(u=(x,y,z,l,w,h,\theta)\)。</p> <p>Anchol与真实值之间的匹配方案为（拿car detection举例），看其在BEV下面的IOU值，当IOU最大时为正样本，或者\(IOU &gt; 0.6\) 为正样本，$IOU&lt;0.45$为负样本，介于\(0.45\)与\(0.6\)之间的丢弃掉。最终的残差形式为下面的函数，其中前两项为分类的损失，后一项为框的拟合程度损失。</p> \[L=\alpha\frac{1}{N_{pos}}\sum_{i}L_{cls}(P_i^{pos}, 1)+\beta\frac{1}{N_{neg}}\sum_jL_{cls}(p_j^neg, 0)+\frac{1}{N_{pos}}\sum_iL_{reg}(\textbf{u}_i,\textbf{u}_i^*)\] <p>训练模型前，论文对数据进行了一定程度的数据增强。对真实的3D标注框，进行旋转，平移，同时去除的碰撞的情况。以及对3D标注框，整体点云分别进行\([0.95~1.05]\)之间不同的缩放。</p> <h3 id="325-模型效果">3.2.5 模型效果</h3> <p>数据援引论文原文，训练和测试均在KITTI上进行，指标为ACC。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/voxelnet%E6%95%88%E6%9E%9C-480.webp 480w, /assets/img/blog/pcd-survey/voxelnet%E6%95%88%E6%9E%9C-800.webp 800w, /assets/img/blog/pcd-survey/voxelnet%E6%95%88%E6%9E%9C-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/voxelnet%E6%95%88%E6%9E%9C.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> VoxelNet模型在KITTI数据集上的效果 </div> <h3 id="326-算法总结">3.2.6 算法总结</h3> <p>VoxelNet 模型属于比较早期的将点云转为 Voxel 作处理的模型（2017），突破性地真正发挥 Voxel 分割点云的作用。此后，在这个思路上，对点云目标检测的效果进行了许多改进。更关键的，论文提出了一种基于体素的编码器VFE（Voxel Feature Encoder），日后几乎所有的3D目标检测模型都采用VFE作为处理点云的第一个步骤，这足以说明VoxelNet所产生的影响之深。</p> <h2 id="33-centerpoint-2021">3.3 CenterPoint-2021</h2> <h3 id="331-论文摘要">3.3.1 论文摘要</h3> <p>由 Utexas 于 2021 年发表论文 “Center-based 3D Object Detection and Tracking”，采用前人改进的骨干网络，将2D CenterPoint 算法推广到 3D CenterPoint 算法。</p> <p>其基本思路是：1. 点云数据通过3D骨骼网络，以Voxel特征描述，提取BEV下的特征图； 2. 基于2D的RCNN检测头寻找目标中心点，以及边框的3D尺寸，3D朝向，速度（stage one），依据中心点以中心特征回归进一步优化预测值（stage two）。其本质是一个two stage, anchor free的算法。</p> <p>在nuScenes上表现出SOTA水准（NDS 65.5，AMOTA 63.8 ），同时在waymo数据集上远好于其他纯lidar方案。作者认为，使用中心点表示，能够大幅减少3D检测的难度，在应对不同朝向上有很好的表现。</p> <p>骨骼网络采用 Voxel-Net 或 PointPillar，将空间分割为长条形的Voxel（Pillar），输入给Two-Stage模型预测。</p> <h3 id="332-创新点">3.3.2 创新点</h3> <p>CenterPoint 提出使用点来代表三维空间中的物体，使用一个关键点检测器来物体中心，以及回归属性（3D size，3D朝向，速度）。在第二阶段，借助点特征来进一步优化这些估计。</p> <p>利用点的表示形式，避免了主干网络去学习旋转不变性。同时引入了CneterNet算法中的注意力机制，提升算法表现。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/centerpoint%E6%B3%A8%E6%84%8F%E5%8A%9B-480.webp 480w, /assets/img/blog/pcd-survey/centerpoint%E6%B3%A8%E6%84%8F%E5%8A%9B-800.webp 800w, /assets/img/blog/pcd-survey/centerpoint%E6%B3%A8%E6%84%8F%E5%8A%9B-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/centerpoint%E6%B3%A8%E6%84%8F%E5%8A%9B.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CenterPoint利用了CenterNet算法的注意力机制 </div> <p>简单来讲，CenterPoint 用 Voxel 描述空间，用 Point 描述物体；结合 2D 目标检测经验，增加单位数据价值；减少 Voxel 计算量，进一步解决“数据处理点多，平均一次扫描获取几十万个数据点”的问题。</p> <h3 id="333-网络结构">3.3.3 网络结构</h3> <p>CenterPoint 网络分为三个层级：3D 骨骼网络 (3D Backbone Network)、区域候选网络 (Region Proposal Network)、区域卷积神经网络 (Regions with CNN features)。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/centerpoint%E7%BB%93%E6%9E%84-480.webp 480w, /assets/img/blog/pcd-survey/centerpoint%E7%BB%93%E6%9E%84-800.webp 800w, /assets/img/blog/pcd-survey/centerpoint%E7%BB%93%E6%9E%84-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/centerpoint%E7%BB%93%E6%9E%84.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CenterPoint算法网络结构 </div> <h4 id="3d-骨干网络">3D 骨干网络</h4> <p>采用骨干网络 (VoxelNet 或 PointPillar)。思路比较类似，将 3D 点云划分为 Voxel，但在$Z$方向上不做划分，感官上类似于“长条形”，用这类 Voxel（Pillar） 描述点云空间，提取点云特征。</p> <p>提取到的特征统称为 map-view feature，\(M\in\mathbb{R}^{W*L*F}\) (一个3D的Tensor，类比图像的\(W*H*C\)，这样就就可以采用图像的方法)。</p> <h4 id="多种检测头---stage-one">多种检测头 - Stage One</h4> <p>3D 骨骼网络输出了点云特征，在 Stage One 通过3种检测头来得到 (1). center heatmap head（检测中心点），(2).regression head （检测中心点偏移+边框）(3). velocity head （检测目标速度）。</p> <ol> <li>Center heatmap head: 与 CenterNet 思路类i，在 backbone 特征后连接一个 heatmap header 用于预测目标中心点。对于\(k\)个label，输出$k$个channel。选取 top100 个热力值的 peak 点作为正负样本的候选。考虑到点云数据稀疏，将gt的框投影到map-view feature中时，以gauss focal loss的方式，扩大对每个gt的高斯渐变范围，其半径为\(\delta =max(f(wl),\tau)\)，增多监督学习中的正样本数量。</li> <li>Regression head: 回归中心点的偏差（中心点归属的voxel存在取整后的误差），以及3DBBox信息，训练阶段仅正样本计算损失，推理时选取 heatmap 峰值，在 regression head 中获取 3D-BBox 的位置。</li> <li>Velocity head: 预测二维的速度，用于tracking，这个需要输入当前时刻与上一时刻的map-view feature，只做检测可以不采用这一环节。预测的方式采用贪心匹配（匈牙利算法，类似SORT），如果目标连续三帧无匹配，则删除映射。</li> </ol> <h4 id="预测---stage-two">预测 - Stage Two</h4> <p>将第一阶段检测到的100个框，每一个框投影回map-view feature，拿到4条边中心点+1个中心点，对应的feature，堆叠起来。 为解决坐标轴对齐问题，采用双线性插值来获取以上点的特征。 最后，整合以上5个特征，输入给MLP计算，预测置信度和回归信息，分类信息在 Stage one 已经解决，得到带有方向的预测bbox。</p> <p>置信度损失计算: 检测框的目标置信度采用下面方式计算，基本上&gt;0.5则目标置信度为1，&lt;0.25则置信度为0，然后结合交叉熵来计算损失。</p> <p>\(L_{score}=-I_t\log(\hat{I}_t)-(1-I_t)\log(1-\hat{I}_t)\) \(I=min(1,max(0,2 \times IoU_t-0.5))\)</p> <p>在推理阶段，置信度的计算方式如下，其中\(\hat{Y}_t=max_{0\le k \le K}\hat{Y}_{p, k}\)和\(\hat{I}_t\)分别表示 stage one 和 stage two 对目标\(t\)的置信度。</p> \[\hat{Q}_t=\sqrt{\hat{Y}_t\times\hat{I}_t}\] <p>边框回归：采用NMS，和2D目标检测的思路基本一致，属于比较经典的做法。</p> <h3 id="334-模型效果">3.3.4 模型效果</h3> <p>数据援引论文原文，训练和测试均在Waymo上进行，指标为mAP和mAPH。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/centerpoint%E8%A1%A8%E7%8E%B0-480.webp 480w, /assets/img/blog/pcd-survey/centerpoint%E8%A1%A8%E7%8E%B0-800.webp 800w, /assets/img/blog/pcd-survey/centerpoint%E8%A1%A8%E7%8E%B0-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/centerpoint%E8%A1%A8%E7%8E%B0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CenterPoint模型对比之前的所有主流模型 </div> <h3 id="335-算法总结">3.3.5 算法总结</h3> <p>单阶段的模型效果很好。在2021年达到SOTA水平（Waymo和nuScenes），性能优于3DSSD，PointPillar。</p> <p>但运行速度较慢，在Titan RTX上实验，Waymo 11FPS， nuScenes 16FPS， 勉强达到实时计算要求。部署在真车上仍需要改进（数据援引原论文）；在最新的论文中显示，平均的latency为96ms (CVPR2023)。</p> <h2 id="34-voxelnext-2023">3.4 VoxelNeXt-2023</h2> <p>由 CUHK 于 2023 年3 月发表论文 “VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking”，真正直接采用点云数据作预测。</p> <h3 id="341-创新点">3.4.1 创新点</h3> <p>目前的三维检测网络，通常使用稀疏卷积来提取特征（出于效率的考虑），借鉴二维的物体检测网络，锚点，或者中心点（CenterPoint网络）被普遍采用。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/voxelnext%E6%B3%A8%E6%84%8F%E5%8A%9B-480.webp 480w, /assets/img/blog/pcd-survey/voxelnext%E6%B3%A8%E6%84%8F%E5%8A%9B-800.webp 800w, /assets/img/blog/pcd-survey/voxelnext%E6%B3%A8%E6%84%8F%E5%8A%9B-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/voxelnext%E6%B3%A8%E6%84%8F%E5%8A%9B.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CenterPoint模型的注意力模块的计算存在不少性能浪费 </div> <p>锚点或者中心点，都是为图像设计的，没有考虑到点云的不规则以及稀疏性，为了应用这些替代的表示方法，主流的检测器将三维稀疏特征转换成二维的稠密特征，然后使用RPN，以及使用稠密的检测头。尽管有效，但这样低效以及复杂。从 CenterPoint 的热力图可以看出，很多部分的 BEVfeature 计算都是浪费的。后续还需要进行 NMS来去除重复的检测。</p> <p>为解决 CenterPoint 热力图和 NMS 重复计算问题，并进一步发掘激光雷达数据稀疏性的特点，提出 VoxelNeXt 模型，采用纯稀疏的 Voxel 网络，直接基于3D点云预测，而不是通过2D数据升维预测。减少了非常多的计算量，让实时的雷达目标检测成为可能。</p> <h3 id="342-网络结构">3.4.2 网络结构</h3> <p>在没有任何其他复杂设计的情况下，通过额外的下采样层可以简单地解决感受野瓶颈不足的问题。点云或体素分布不规则，通常分散在3D对象的表面，而不是中心或内部，因此直接基于体素而不是手工制作的锚或中心来预测3D Bbox。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/voxelnext%E7%BB%93%E6%9E%84-480.webp 480w, /assets/img/blog/pcd-survey/voxelnext%E7%BB%93%E6%9E%84-800.webp 800w, /assets/img/blog/pcd-survey/voxelnext%E7%BB%93%E6%9E%84-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/voxelnext%E7%BB%93%E6%9E%84.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 对比以往常规的3D目标检测，VoxelNeXt的网络结构非常简单 </div> <p>VoxelNeXt 网络包含了4个实现细节：1) 骨干网络，2) 将3D稀疏体素压缩成 2D 稀疏体素，3）Sparse max pooling / NMS，4）用3x3 sparse conv或FC来预测物体。</p> <h4 id="骨干网络">骨干网络</h4> <p>一般情况下，简单稀疏的 CNN 骨干网络有4个阶段，其特征步长为 \(\{1,2,4,8\}\)，将其输出的稀疏特征命名为\(\{F_1,F_2,F_3,F_4\}\)。目前的特征无法描述和预测，尤其是大型的对象（占多个Voxel）。额外多两次下采样，得到步长为\(\{16,32\}\)的特征\(\{F_5,F_6\}\)。</p> <p>将最后三个步骤\(\{F_4,F_5,F_6\}\)的稀疏特征进行拼接，只需要作简单的稀疏串联，而不需要其他复杂的参数化层，使其空间对其到\(F_4\)特征空间。其中，对于阶段$i$，$F_i$是一组单独的特征\(f_p\)，\(p\in P_i\)是 3D 空间的点，坐标为\(\{x_p,y_p,z_p\}\)。</p> <p>\(F_c=F_4\cup(F_5\cup F_6)\) \(P_6'=\{(x_p\times2^2,y_p\times2^2,z_p\times2^2) | p\in P_6 \}\) \(P_5'=\{(x_p\times2^1,y_p\times2^1,z_p\times2^1) | p\in P_6 \}\) \(P_c=P_4\cup(P_5' \cup P_6')\)</p> <p>在两次额外下采样下做稀疏特征串联后，有效感受范围扩大，预测更准确，且不需要太多的计算。</p> <h4 id="体素压缩">体素压缩</h4> <p>3D 体素映射到 2D：这一步骤中，将稀疏特征转换为密集特征，压缩\(z\)方向，将 3D 体素特征压缩为密集的 2D 特征图。VoxelNet 中发现，2D 的稀疏特征对预测有效，不单单只是抑制模型收敛。在 VoxelNeXt 中，对高度的压缩只是以体素为对单位，映射在统一平面上，对于同一区域的特征累加。数学建模以上过程，如下所示：</p> <p>\(\bar{P}_c=\{(x_p,y_p)\|p\in P_c\}\) \(\bar{F}_c=\{\sum_{p\in S_{\bar{p}}}{f_p} \| \bar{p}\in\bar{P}_c\}\)</p> <p>其中\(S_{\bar{p}}=\{p\|x_p=x_{\bar{p}},y=y_{\bar{p}, p\in P_c} \}\)，包含映射在 2D 平面上的体素。</p> <p>体素裁减：由于网络完全基于体素本身，而 3D 点云中含有大量冗余的背景点，对预测有很大的不利，因此需要对映射后的 2D 体素做裁剪。沿着下采样层逐渐修剪不相关的体素，根据SPS Conv，抑制了具有小特征量值的体素的膨胀。将抑制比设为0.5，仅对特征幅度\(\|f_p\|\)（在通道维度上平均）排在前一半的体素进行扩张。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/voxelnext%E4%B8%8B%E9%87%87%E6%A0%B7-480.webp 480w, /assets/img/blog/pcd-survey/voxelnext%E4%B8%8B%E9%87%87%E6%A0%B7-800.webp 800w, /assets/img/blog/pcd-survey/voxelnext%E4%B8%8B%E9%87%87%E6%A0%B7-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/voxelnext%E4%B8%8B%E9%87%87%E6%A0%B7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 类似高维合并后的下采样式体素裁减 </div> <p>体素选择：论文没有采用常用的 NMS 方法，不依赖于密集的特征图，而是基于3D CNN骨干网络的输出进行预测。在训练过程中，我们将离每个注释边界框中心最近的体素指定为正样本。我们使用焦点损失进行监督。 在推理时，由于特征足够稀疏，可以直接用简单 max pooling 选择具有空间局部最大特征的体素，节省检测头的计算。</p> <p>回归预测： 回归方法与 CenterPoint 类似，简单地使用核大小为3的全连通层或 3×3 子流形稀疏卷积层进行预测，而不需要其他复杂的设计。论文发现，3×3 稀疏卷积比全连接层产生更好的结果，但目前缺少数学上的理论支撑。 同样的，执行 3D Tracking 任务时的思路和 CenterPoint 类似，使用体素关联来包含更多与查询体素位置匹配的轨迹，不多赘述。</p> <h3 id="343-模型效果">3.4.3 模型效果</h3> <p>对比CenterPoint，训练和测试在Waymo数据集上，指标为IoU。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/voxelnext%E5%AF%B9%E6%AF%94centerpoint-480.webp 480w, /assets/img/blog/pcd-survey/voxelnext%E5%AF%B9%E6%AF%94centerpoint-800.webp 800w, /assets/img/blog/pcd-survey/voxelnext%E5%AF%B9%E6%AF%94centerpoint-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/voxelnext%E5%AF%B9%E6%AF%94centerpoint.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> VoxelNeXt解决了CenterPoint模型中注意力浪费的部分，性能得到提升 </div> <p>对比以往常规思路的3D点云目标检测算法，训练和测试在Waymo数据集上。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/voxelnext%E8%A1%A8%E7%8E%B0-480.webp 480w, /assets/img/blog/pcd-survey/voxelnext%E8%A1%A8%E7%8E%B0-800.webp 800w, /assets/img/blog/pcd-survey/voxelnext%E8%A1%A8%E7%8E%B0-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/voxelnext%E8%A1%A8%E7%8E%B0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> VoxelNeXt与其他模型的效果对比 </div> <h3 id="344-论文总结">3.4.4 论文总结</h3> <p>论文思想来源于 CenterPoint，从 3D 的角度考虑点云特征，没有手动构建而是直接送入网络学习，同时很好的发挥了点云数据的稀疏性，减少计算量的同时优化表现，在多个数据集上都做到SOTA。</p> <p>实际表现，可能在训练上有一定难度。由于模型本身由于没有忽略$z$方向上的信息，对高度敏感，易于在不同高度上检测出物体。</p> <p>但模型的训练存在比较明显的问题，论文中采用知识蒸馏的方案，同时训练了比较长的时间，得到了比较好的预期结果。实际在本地训练时，耗时巨大且提升不明显，是需要一些高级的模型训练方案才可以得到预期效果的检测模型。</p> <h1 id="4-实验复现">4. 实验复现</h1> <h2 id="41-实验设计">4.1 实验设计</h2> <p>准备两组数据集\(D_{sim}\)和\(D_{real}\)，分别在仿真环境Carla和真实环境（设备Velodyne—VLP16，园区内）采集。分别训练VoxelNet、PointPillar、CenterPoint以及VoxelNeXt模型(epochs=160, batch size=18，split=0.2)。观察训练时间以及表现效果，数据集的详细信息如下：</p> <table> <thead> <tr> <th><strong>Dataset</strong></th> <th><strong>Size</strong></th> <th><strong>Frame</strong></th> <th><strong>Instances</strong></th> <th><strong>class</strong></th> <th><strong>detail</strong></th> </tr> </thead> <tbody> <tr> <td>\(D_{sim}\)</td> <td>20.03G</td> <td>3971</td> <td>75294</td> <td>Car, Truck, Van, Pedestrian, Cyclist</td> <td>city, highway</td> </tr> <tr> <td>\(D_{real}\)</td> <td>16.77G</td> <td>4068</td> <td>66329</td> <td>Vehicle, Pedestrian</td> <td>Campus</td> </tr> </tbody> </table> <h2 id="42-实验设备">4.2 实验设备</h2> <ul> <li>CPU: Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz</li> <li>GPU: NVIDIA TITAN V x 6</li> <li>OS: Ubuntu 22.04.2 LTS</li> <li>MEM: 453 G</li> </ul> <h2 id="43-实验结果">4.3 实验结果</h2> <p>以下为仿真数据集试验结果。</p> <table> <thead> <tr> <th><strong>样本类</strong></th> <th><strong>VoxelNet</strong></th> <th><strong>PointPillar</strong></th> <th><strong>CenterPoint</strong></th> <th><strong>VoxelNeXt</strong></th> </tr> </thead> <tbody> <tr> <td>Car</td> <td>44.68</td> <td>60.28</td> <td><strong>64.09</strong></td> <td>24.09</td> </tr> <tr> <td>Truck</td> <td>52.39</td> <td><strong>71.27</strong></td> <td>68.32</td> <td>38.23</td> </tr> <tr> <td>Van</td> <td>52.46</td> <td>54.11</td> <td><strong>71.09</strong></td> <td>30.48</td> </tr> <tr> <td>Pedestrian</td> <td>30.22</td> <td>40.87</td> <td><strong>48.05</strong></td> <td>9.71</td> </tr> <tr> <td>Cyclist</td> <td>29.14</td> <td><strong>56.30</strong></td> <td>52.26</td> <td>12.33</td> </tr> <tr> <td>训练时间</td> <td>3h33min</td> <td>4h17min</td> <td>5h39min</td> <td>6h13min</td> </tr> </tbody> </table> <p>以下为真实采集数据集下实验结果。</p> <table> <thead> <tr> <th><strong>class</strong></th> <th><strong>VoxelNet</strong></th> <th><strong>PointPillar</strong></th> <th><strong>CenterPoint</strong></th> <th><strong>VoxelNeXt</strong></th> </tr> </thead> <tbody> <tr> <td>Vehicle</td> <td>22.03</td> <td>26.75</td> <td><strong>28.09</strong></td> <td>15.92</td> </tr> <tr> <td>Pedestrian</td> <td>9.51</td> <td>11.20</td> <td><strong>14.69</strong></td> <td>4.32</td> </tr> <tr> <td>time cost</td> <td>3h04min</td> <td>3h45min</td> <td>4h12min</td> <td>6h27min</td> </tr> </tbody> </table> <h2 id="44-结果分析">4.4 结果分析</h2> <ol> <li>就模型训练时间而言，随算法复杂度提高，训练时间逐渐增加。</li> <li>就模型表现（mAP）而言，在仿真数据集上，CenterPoint算法与PointPillar不相上下。在真实数据集上，CenterPoint算法表现效果最好。</li> <li>由于真实数据采集所用的设备Velodyne-VLP16是16线激光雷达，对于目标物体的描述很不清晰，导致训练结果很差，与仿真数据（128线模拟激光雷达）所训练的模型表现相差甚远。</li> </ol> <h1 id="5-总结">5. 总结</h1> <p>从PointNet到VoxelNeXt算法，激光雷达3D目标检测算法的发展越见成熟。从提出骨干网络，到VFE和RPN的首个处理过程，到转为two stage算法，到引入注意力机制，激光雷达3D目标检测算法越来越复杂，效果也逐渐提升。然而，离数据本身的特征看似却越来越远。VoxelNeXt的提出无疑是给研究者们一个很好的启示——利用好数据本身的特质，即使是简单的网络结构依然能有非常好的效果，看起来是揭示了激光雷达3D点云目标检测的又一个发展新趋势。</p> <p>本篇文章从最初的激光雷达3D点云目标检测的诞生讲起，对其发展历程中的几个重大的里程碑式的算法进行了详细的介绍与分析，在本地部署和测试算法性能，最后提出了些许个人看法。希望能够对激光雷达3D点云目标检测算法的初学者带来一些帮助。</p>]]></content><author><name></name></author><category term="note"/><category term="AD"/><category term="survey"/><summary type="html"><![CDATA[Please note that it is written in Simplified Chinese]]></summary></entry></feed>