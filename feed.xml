<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jonaruthardt.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jonaruthardt.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-16T15:27:13+00:00</updated><id>https://jonaruthardt.github.io/feed.xml</id><title type="html">blank</title><subtitle>Academic website of Zexuan Jia. </subtitle><entry><title type="html">3D Traffic Vulnerable Group Detection in Simulation-driven Autonomous Driving</title><link href="https://jonaruthardt.github.io/blog/2023/Sim2det3D/" rel="alternate" type="text/html" title="3D Traffic Vulnerable Group Detection in Simulation-driven Autonomous Driving"/><published>2023-10-08T15:12:00+00:00</published><updated>2023-10-08T15:12:00+00:00</updated><id>https://jonaruthardt.github.io/blog/2023/Sim2det3D</id><content type="html" xml:base="https://jonaruthardt.github.io/blog/2023/Sim2det3D/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Autonomous driving is one of the most exciting topics in the fields of machine learning and deep learning. In recent years, the technology behind autonomous driving has advanced rapidly in both academia and industry. The various aspects of autonomous driving can be divided into three main modules: perception, prediction, and decision-making. Due to the data-driven nature of deep learning models, effective algorithms require high-quality data sets. If these high standards are not met, the desired outcomes are unlikely to be achieved. Currently, it is acknowledged that in the decision-making module, training on 1 million kilometers of data can lead to better results. However, no similar benchmark exists for the perception module.</p> <p>Previous 3D Lidar detection algorithms often overlooked vulnerable traffic groups, such as pedestrians and cyclists. We aim to propose a straightforward strategy or framework that maximizes the utility of unit data frames within enhanced datasets, thereby improving the algorithm’s detection of vulnerable traffic groups.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/framework-480.webp 480w, /assets/img/blog/sim2det/framework-800.webp 800w, /assets/img/blog/sim2det/framework-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/framework.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The simple main pipeline of this project </div> <p>Unfortunately, Lidar sensors are expensive, making them unaffordable for our project team. Furthermore, collecting and annotating Lidar data poses significant challenges. Compared to traditional data or image data for autonomous driving, Lidar data is often “too sparse, abstract in description, and difficult to visualize.” This is especially true for identifying traffic-disadvantaged groups, as labeling them based on real data is particularly challenging.</p> <p>To address these issues, our project employs a simulation environment for data collection and preparation. We have developed a set of tools to automatically collect and annotate data within this simulation environment, which allows us to directly extract the 3D positions of targets for annotation. Additionally, given the low frequency of vulnerable groups in existing datasets, we constructed simple scenarios in high dimensions to increase the sample size of these groups.</p> <p>In summary, to tackle the challenges posed by the small and unclear samples of vulnerable traffic groups (pedestrians and cyclists) in traditional datasets, our project has built custom collection tools based on a simulation environment. We also proposed a strategic framework to enhance data collection. To verify its effectiveness, we tested our custom dataset across multiple algorithms, observing a significant improvement in the detection of vulnerable traffic groups.</p> <h1 id="related-work">Related Work</h1> <p>CARLA is a powerful open-source simulator designed for autonomous driving research. It can create a virtual urban environment and simulate various sensors, including cameras, LiDAR, and mmWave radar, to provide essential data. Many researchers have developed their self-driving systems within the Carla environment. By utilizing established object detection algorithms such as YOLO and Faster R-CNN to process the data generated by Carla, they can effectively implement object detection in their systems. The same applies to object tracking, where algorithms like GOTURN and Deep SORT can be employed to achieve successful tracking.</p> <p>For this project, we will use the open-source 3D point cloud detection algorithm training framework, OpenPCDet. This framework is currently a popular and lightweight option for point cloud algorithm training and supports a variety of network architectures.</p> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Car@R11</strong></th> <th><strong>Pedestrian@R11</strong></th> <th><strong>Cyclist@R11</strong></th> <th><strong>Dataset</strong></th> </tr> </thead> <tbody> <tr> <td>PointPillar</td> <td>77.28</td> <td>52.29</td> <td>62.68</td> <td>KITTI</td> </tr> <tr> <td>SECOND</td> <td>78.62</td> <td>52.98</td> <td>67.15</td> <td>KITTI</td> </tr> <tr> <td>Voxel R-CNN</td> <td>84.54</td> <td>-</td> <td>-</td> <td>KITTI</td> </tr> <tr> <td>BEVFusion</td> <td>67.75</td> <td>-</td> <td>-</td> <td>nuScenes</td> </tr> <tr> <td>CenterPoint</td> <td>78.08</td> <td>49.74</td> <td>67.22</td> <td>ONCE</td> </tr> <tr> <td>Voxel NeXt</td> <td>30.05</td> <td>-</td> <td>-</td> <td>Argoverse2</td> </tr> </tbody> </table> <p>Among these, we chose the PointPillar model for experiments to verify that our method has improved the detection effect of if groups. Voxel R-CNN was selected for experiments to verify that the traditional vehicle detection effect has also been improved.</p> <h1 id="methodology">Methodology</h1> <h2 id="co-simulation">Co-simulation</h2> <p>This section primarily focuses on creating the simulation scene. In the Carla map, there are numerous static vehicles that are integral to the map design and are not generated through program control. Consequently, their 3D positions do not appear in memory and cannot be annotated. During the training and verification phases, the model may detect these static vehicles but might misclassify them due to the lack of annotations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/ue41-480.webp 480w, /assets/img/blog/sim2det/ue41-800.webp 800w, /assets/img/blog/sim2det/ue41-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/ue41.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/ue42-480.webp 480w, /assets/img/blog/sim2det/ue42-800.webp 800w, /assets/img/blog/sim2det/ue42-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/ue42.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Operate source code version Carla in the Unreal4 </div> <p>Therefore, we first operate in the source version of Carla and use the Unreal4 toolkit to eliminate static vehicles. After that, we used the Carla-Apollo Bridge to let Apollo take over Carla’s dynamic scene settings and perform visual operations in DreamViewer.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/cosim1-480.webp 480w, /assets/img/blog/sim2det/cosim1-800.webp 800w, /assets/img/blog/sim2det/cosim1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/cosim1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/cosim2-480.webp 480w, /assets/img/blog/sim2det/cosim2-800.webp 800w, /assets/img/blog/sim2det/cosim2-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/cosim2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Operate source code version Carla in the Unreal4 </div> <h2 id="problem-definition">Problem Definition</h2> <p>Given a 3D point cloud, \(\mathbf{P}=\{p_1,p_2,...,p_n\}\), which represents the set of measured points, \(p_i=(x_i,y_i,z_i)\), and presents a snapshots of the surroundings. For objects, \(\mathbf{O}=\{o_1,o_2,...,o_m\}\), represents the set of all objects in point cloud (vehicle, traffic light, pedestrian, etc.), using KITTI format, where \(o_j=(x_j,y_j,z_j,w_j,h_j,l_j,r_j)\).</p> <p>Use \(S_{keep}\) to measure whether to keep the Lidar data.</p> <p>\(S_{keep} \equiv \mathcal{E} (P) \cdot \mathcal{P}(\rho(O),\tau(O))\) where $\mathcal{E}$ decides whether to keep the entire point cloud, and $\mathcal{P}$ decides whether to keep the collected target data.</p> <p>Use \(S_{value}\) to represent the value of current Lidar data for the trainning model,</p> \[S_{value}\equiv \phi(P, O) \cdot \psi(O)\] <p>where \(\phi(P, O)\) is the perception distance term, used to describe the relationship between the effective perception radius and the farthest perception radius; \(\psi(O)\) is the prediction accuracy term, used to describe the perception How accurate is the prediction of objects within the radius.</p> <h2 id="pedestrian-control-algorithm">Pedestrian Control Algorithm</h2> <p>For pedestrian control in the Co-Simulate link, there is the following algorithm. It controls pedestrian behavior during the time between two timestamps, where $pos$ is the position, $range$ is perception range (only 160 degrees in front of the eyes), $towards$ is the absolute angle of orientation, and $speed$ is the speed of a pedestrian, respectively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/code-480.webp 480w, /assets/img/blog/sim2det/code-800.webp 800w, /assets/img/blog/sim2det/code-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/code.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Simple Pedestrian control pesudo code </div> <h2 id="feature-upgrade">Feature Upgrade</h2> <p>The following is the entire process of our improved VFE processing stage.</p> <ul> <li>De-mean encoding \((P, M, 4)\xrightarrow{}(P, M, 3)\) for each point in Pillar</li> <li>Decentralize the effective points in each Pillar \((P, M, 4)\xrightarrow{} (P, M, 2)\)</li> <li>Mask merge coding: Combine the original \((P, M, 4)\) with the above two codes cat to get the vector of \((P, M, 9)\). There are two points to note here: <ul> <li>Only valid points (\(n\) points per pillar) are operated in each Pillar. If the number of valid points is insufficient, zero will be added, if there are too many, random sample will be used;</li> <li>In the code, the 9-dimensional encoding vector is The first 2 dimensions are replaced by decentralized encoded vectors</li> </ul> </li> <li>Convolution kernel pooling: \((P, M, 9)\xrightarrow{}(P, M, 64)\) and \((P, 64)\)</li> <li>Pillarscatter: Go to the 2D feature map of \((X/vsize, Y/vsize)\) and get the feature map of \((64, X/vsize, Y/vsize)\).</li> </ul> <p>The PointPillar model utilizes a method that differs from Voxel in describing point clouds by employing Pillars, which disregards certain information along the Z-axis. During the VFE encoding process, since the model does not take this information into account, it can also omit it during encoding. This approach enhances coding speed and reduces both training and inference times.</p> <h1 id="experiment">Experiment</h1> <p>A total of three data sets were collected, of which A did not use the scenes we built, and B and D used the scenes we built. Each data set consists of 5 subsets, and the number of “vehicles (including cyclists) and pedestrians” in each subset are \((50,25)\), \((75,37)\), \((100,50)\), \((125,62)\) and \((150,75)\).</p> <table> <thead> <tr> <th><strong>Dataset name</strong></th> <th><strong>Total Frames</strong></th> <th><strong>Map</strong></th> <th><strong>Detail</strong></th> </tr> </thead> <tbody> <tr> <td>A</td> <td>987</td> <td>Town05</td> <td>City</td> </tr> <tr> <td>B</td> <td>902</td> <td>Town02</td> <td>Town</td> </tr> <tr> <td>D</td> <td>988</td> <td>Town06</td> <td>Highway</td> </tr> <tr> <td>V</td> <td>375</td> <td>-</td> <td>Random select from A,B, D</td> </tr> </tbody> </table> <p>Then, use datasets A, B, and D to train on PointPillar and Voxel R-CNN, respectively. Use epoch = 160, batch size = 18, dynamically adjust the learning rate, and set Random seed = 114. This results in a total of 6 models. All model training is performed on the server, and the server parameters are as follows.</p> <ul> <li>CPU: Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz</li> <li>GPU: NVIDIA TITAN V \(\times \ 6\)</li> <li>OS: Ubuntu 22.04.2 LTS</li> <li>MEM: 453 G</li> </ul> <h1 id="results">Results</h1> <p>Via TensorBoard tool, the following is the exported curve of the loss decreasing as the step increases.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/loss_pp-480.webp 480w, /assets/img/blog/sim2det/loss_pp-800.webp 800w, /assets/img/blog/sim2det/loss_pp-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/loss_pp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/loss_vr-480.webp 480w, /assets/img/blog/sim2det/loss_vr-800.webp 800w, /assets/img/blog/sim2det/loss_vr-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/loss_vr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Training loss curves of Pointpillars (left) and Voxel R-CNN (right). </div> <p>The following are the evaluation results of Pointpillars after training in OpenPCDet. The indicator uses mAP70, that is, mAP above 70 is calculated as correct recognition.</p> <table> <thead> <tr> <th><strong>Dataset</strong></th> <th><strong>Car</strong></th> <th><strong>Truck</strong></th> <th><strong>Van</strong></th> <th><strong>Pedestrian</strong></th> <th><strong>Cyclist</strong></th> </tr> </thead> <tbody> <tr> <td>A</td> <td>53.86</td> <td>68.32</td> <td>52.78</td> <td>38.08</td> <td>45.11</td> </tr> <tr> <td>B</td> <td>60.28</td> <td>71.27</td> <td>54.11</td> <td>40.87</td> <td><strong>56.30</strong></td> </tr> <tr> <td>D</td> <td>64.09</td> <td>68.59</td> <td>71.09</td> <td><strong>48.05</strong></td> <td>52.26</td> </tr> </tbody> </table> <p>Similarly, the result of Voxel-RCNN is shown as the followed.</p> <table> <thead> <tr> <th><strong>Dataset</strong></th> <th><strong>Vehicle</strong></th> </tr> </thead> <tbody> <tr> <td>A</td> <td>8.63</td> </tr> <tr> <td>B</td> <td><strong>64.28</strong></td> </tr> <tr> <td>D</td> <td>63.21</td> </tr> </tbody> </table> <p>Visual display of some data of the model on the test set.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pd1-480.webp 480w, /assets/img/blog/sim2det/pd1-800.webp 800w, /assets/img/blog/sim2det/pd1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pd1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pd1-1-480.webp 480w, /assets/img/blog/sim2det/pd1-1-800.webp 800w, /assets/img/blog/sim2det/pd1-1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pd1-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pd2-480.webp 480w, /assets/img/blog/sim2det/pd2-800.webp 800w, /assets/img/blog/sim2det/pd2-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pd2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pd2-1-480.webp 480w, /assets/img/blog/sim2det/pd2-1-800.webp 800w, /assets/img/blog/sim2det/pd2-1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pd2-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pv1-480.webp 480w, /assets/img/blog/sim2det/pv1-800.webp 800w, /assets/img/blog/sim2det/pv1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pv1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/sim2det/pv1-1-480.webp 480w, /assets/img/blog/sim2det/pv1-1-800.webp 800w, /assets/img/blog/sim2det/pv1-1-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/sim2det/pv1-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualization of different models trained on different data sets under the numbered data frames shown </div> <p>It is not difficult to find that the PointPillar model can basically accurately detect small objects in the distance. Voxel R-CNN has correctly and completely detected all vehicles in this scene, even those with severe occlusion.</p> <h1 id="conclusions">Conclusions</h1> <p>In this prpject, we address the current oversight of vulnerable traffic groups, such as pedestrians and cyclists, in 3D detection algorithms. We propose a project framework utilizing the Carla simulation environment, which encompasses scene construction, data preparation, and model training. We conducted experiments using the PointPillar algorithm and the Voxel R-CNN algorithm within the OpenPCDet framework. The experimental results demonstrate that our approach is more effective, significantly enhancing the 3D detection capabilities for vulnerable traffic groups while also improving performance in general vehicle detection tasks.</p>]]></content><author><name></name></author><category term="course-work"/><category term="AD"/><summary type="html"><![CDATA[Introduces a new joint simulation strategy that combines Carla and Apollo to enhance data quality and improve model performance.]]></summary></entry><entry><title type="html">A short review of point cloud detection algorithms - From PointNet to VoxelNeXt (Simplified Chinese)</title><link href="https://jonaruthardt.github.io/blog/2023/pcd-survey/" rel="alternate" type="text/html" title="A short review of point cloud detection algorithms - From PointNet to VoxelNeXt (Simplified Chinese)"/><published>2023-08-17T15:12:00+00:00</published><updated>2023-08-17T15:12:00+00:00</updated><id>https://jonaruthardt.github.io/blog/2023/pcd-survey</id><content type="html" xml:base="https://jonaruthardt.github.io/blog/2023/pcd-survey/"><![CDATA[<h1 id="摘要">摘要</h1> <p>近年来，激光雷达点云目标检测算法在自动驾驶、机器人感知等领域中扮演着重要角色。本综述旨在回顾激光雷达点云目标检测算法的发展历程，重点关注从经典的PointNet算法到最新的VoxelNext算法的演进过程。我将从算法的基本原理、方法优劣势、实验结果等方面综合评述各个算法，并提出当前研究的挑战和未来发展方向。作为激光雷达点云目标检测算法的学习启蒙。</p> <h1 id="引言">引言</h1> <h2 id="背景">背景</h2> <p>自动驾驶和机器人技术的快速发展推动了激光雷达点云目标检测算法的研究。激光雷达点云作为一种重要的三维感知数据，具有高精度和丰富的空间信息，因此被广泛应用于目标检测任务中。感知环节，又可以细分为多个模块。具体来讲，从 Detection, Postion, On-board Map 三个方面考虑。本文主要考虑的是其中的 Detection 方面。该环节主要考虑如何实时检测障碍物、目标物等。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/%E5%BC%95%E8%A8%80-480.webp 480w, /assets/img/blog/pcd-survey/%E5%BC%95%E8%A8%80-800.webp 800w, /assets/img/blog/pcd-survey/%E5%BC%95%E8%A8%80-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/%E5%BC%95%E8%A8%80.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 3D激光雷达点云目标检测 </div> <ul> <li>2016年，PointNet首次提出点云分割的基本思路、骨干网络，以严谨而全面的实验验证算法各个环节的有效性。</li> <li>2017年，VoxelNet以 PointNet 为基础，用 Voxel 描述点云空间，提出VFE（Voxel Feature Encoder，体素特征提取器），作为往后几乎一切点云目标检测算法的首要环节。</li> <li>2018年，SECOND使用改进的稀疏卷积和新的损失函数，对 VoxelNet 进行了一些优化和完善，采用改良的稀疏卷积，效果改善显著。</li> <li>2019年，PointPillar 将VoxelNet中Voxel改为长条形的Pillar，采用SECOND中的稀疏卷积核，几乎不影响性能的情况下极大地提升了性能，是工业界常用的算法之一。</li> <li>2020年，Voxel R-CNN基于voxel特征，设计了3D点云目标检测的two stage网络，提升了些许检测精度。</li> <li>2021年，CenterPoint继承 PointPillar 算法的Pillar化思路，并结合2D的CenterNet算法，引入注意力机制，看到了在3D检测上采取2D检测策略的优势性，进一步提升two stage系列算法性能。</li> <li>2023年，Voxel NeXt回归到点云数据本质，采用纯稀疏的卷积网络，解决了CenterPoint中注意力浪费的部分，并结合Segment Anything开发出一套强鲁棒检测算法驱动的自动标注系统。</li> </ul> <p>以上是近年来一些代表性比较强的点云目标检测算法的发展历程。本文将对其中PointNet，VoxelNet，CenterPoint 以及 VoxelNeXt四个算法进行详细的介绍，并分析算法的创新点，以及一些思考。</p> <h2 id="问题定义">问题定义</h2> <p>3D目标检测是自动驾驶的一个基础部分，3D目标检测网络通常输入稀疏点云，输出物体三维位置以及类别。硬件设备常常选用激光雷达Lidar。</p> <p>对于整个点云数据帧$F$，其内部有点集\(P = \{p_1,p_2,...,p_n\}, p_k=\{x_k,y_k,z_k,I_k\} , p_k \in P\)（\(I\)表示点的强度信息）。对\(P\)执行检测算法\(A\)，返回预测目标集\(D=\{d_1,d_2,...,d_m\}, d_i = \{x_i,y_i,z_i,h,w,l,rot,lab\}, d_i\in D\)，其中\(x,y,z\)表示3D框的中心点，\(h,w,l\)表示3D框的高、宽、长，\(rot\)表示在6轴坐标系下的\(yaw\)旋转角，\(lab\)表示3D框的语义信息，即预测目标的类别。</p> <h2 id="评估标准">评估标准</h2> <h3 id="metrics-based-on-average-precision">Metrics based on Average Precision</h3> <p>检测算法的评估，一般采用 IoU (Inner of Union) 作为评价 BBox 和 Ground-Truth 的重合程度。数学建模为，对于同一个对象$O$，取算法预测框$D$与真值框$G$ (Ground Truth) ，按如下公式计算：</p> \[IoU = \frac{area(D)\cap area(G)}{area(D)\cup area(G)}\] <p>定义 \(IoU&gt;0.5\)为真阳性 \(TP\) (True Positive)，\(IoU\le0.5\)为假阳性\(FP\)（False Positive）；定义真值未检测的为假阴性$FN$(False Negivate)，定义检测出但真值不存在的为真阴性\(TN\)(True Positive)。根据以上几个定义，可以计算出模型整体的准确度$P$(Precision) 与召回率\(R\)(Recall)，并绘制成 \(P-R\) 曲线。</p> \[P=\frac{TP}{FP+FN}\ \ \ R=\frac{TP}{TP+FN}\] <p>计算曲线的下积分面积\(AP\)(Average Precision)。近似地，用以下公式估计\(AP\)的值，表示为对于此次测试的数据，模型整体的平均准确度。对所有测试数据求平均，求的\(mAP\)用于评估整个模型在测试集上的效果。</p> <p>\(AP = \sum_{k=0}^{k = n - 1}[Recall(k) - Recall(k-1)] \times Precision(k)\) \(mAP = \frac{1}{n} \sum_{k = 1}^{k = n}AP_k\)</p> <p>进一步地，更改\(IoU\)的阈值，从 \(0.5\)以\(0.05\)为单位递增至\(0.95\)，计算每个阈值下的\(mAP\)，更严格地评价模型效果，将该指标称为$mAP50-95$。 对于点云数据，由于其数据特有的稀疏性，通过考虑地平数据点比较稀疏和离散，不好作特征提取，由面上的 2D 中心距离而不是基于联合的亲和力的交集来定义匹配。具体来说，将预测与具有最小中心距离且达到特定阈值的地面真实对象进行匹配。对于给定的匹配阈值，我们通过整合召回率与精度曲线（召回率和精度 &gt; 0.1）来计算$AP$。我们最终对 \(\{0.5,1,2,4\}\) 米的匹配阈值进行平均，并计算各个类别的平均值。</p> <h3 id="metrics-based-on-true-positive">Metrics based on True Positive</h3> <p>依据nuScenes提出的标准，定义一组真阳性 (TP) 的指标，用于测量平移、尺度、方向、速度和属性错误。所有\(TP\)指标都是在匹配时使用$2m$中心距的阈值计算的，并且它们都被设计为正标量。</p> <p>每个类别的匹配和评分都是独立进行的，每个指标都是每个达到 10% 以上的召回水平的累积平均值的平均值。如果特定类别未达到 10\% 的召回率，则该类别的所有 \(TP\) 错误都将设置为 1。我们定义以下 \(TP\) 错误：</p> <ul> <li>平均平移误差 ATE (Average Translation Error)：二维欧几里得中心距离（以米为单位）；</li> <li>平均尺度误差 ASE (Average Scale Error)：在对齐中心和方向后计算为\(1 - IOU\) ；</li> <li>平均方向误差 AOE(Average Orientation Error)：预测与地面实况之间的最小偏航角差异（以弧度为单位）。对于所有类别，方向误差均以 360 度进行评估，障碍除外，障碍物仅以 \(180\)度进行评估。忽略锥体的方向错误；</li> <li>平均速度误差 AVE (Average Velocity Error)：绝对速度误差，以 \(m/s\) 为单位。障碍物和锥体的速度误差被忽略；</li> <li>平均属性误差 AAE (Average Attribute Error)：计算为\(1 - acc\)，其中 $acc$ 是属性分类精度。障碍物和锥体的属性错误被忽略。</li> </ul> <p>以上5个标准与mAP加权求和，即nuScenes定义的NDS (nuScenes Detection Score) 标准，即</p> \[\text{NDS} = 1/10[5\text{mAP}+ {\sum}_{\text{mTP}\in\mathbb{TP}} (1-\min(1, \text{mTP}))]\] <p>以上所有误差都 &gt;= 0，但是，对于平移误差和速度误差，误差是无界的，并且可以是任何正值。此类 $TP$ 指标是按类定义的，然后我们取类的平均值来计算 \(mATE, mASE, mAOE, mAVE,mAAE\)。</p> <h2 id="任务难点">任务难点</h2> <p>由于深度学习数据驱动的特性，就算法训练而言，在数据获取上，相比与其他数据，3D点云数据集的制备和处理非常困难。</p> <p>对比于 2D 图像数据，处理点云数据具有以下几个难点。在后文中，会分别简要提到如何克服以下几个困难。数据点比较稀疏和离散，不好作特征提取，噪声不好区别；具有更多自由度，不好处理（更多的尺寸种类，相对于坐标周有多个维度的旋转）；数据标注困难，每个Ground Truth（真值框）需要八个点位的标注。</p> <h2 id="常见的公开数据集">常见的公开数据集</h2> <p>针对数据获取的困难，往往采用一些常见的开源点云数据集进行训练。</p> <ul> <li>KITTI 3D：具备1.5w帧数据，8w个标注的目标；</li> <li>nuScenes：具备39w帧雷达，140w个标注目标，23类；</li> <li>Waymo：具备200w帧，2260w个标注目标，4类+23类；</li> <li>Argoverse2：具备1000个场景，每帧图像平均有75个目标物，30个类别；</li> <li>Dense: 不同天气下12000个样本和浓雾中的1500个样本，少见的极端天气数据集；</li> <li>SUScape：南方科技大学与Intel公司合作，在深圳采集的数据集，具有40+类别，有效平均实例密度为nuScene的10倍。</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/nuScene%E8%BD%A6-480.webp 480w, /assets/img/blog/pcd-survey/nuScene%E8%BD%A6-800.webp 800w, /assets/img/blog/pcd-survey/nuScene%E8%BD%A6-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/nuScene%E8%BD%A6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> nuScenes数据集车辆传感器设置 </div> <p>如果想要自己制备数据集用来训练模型，往往采用以下的方法。</p> <ul> <li>自制真实数据集：机器人采集真实数据，通过 Segement Anything，SUSTechPoint 等工具辅助标注；</li> <li>仿真采集数据集：Carla 等仿真环境，数据标注来源于对仿真环境真值的处理。目前有一种主动式的方针采集思路CARLA-ADA，能够制备比较有效的仿真数据集。</li> </ul> <h1 id="算法介绍">算法介绍</h1> <h2 id="pointnet-2016">PointNet-2016</h2> <p>PointNet 和 PointNet++ 最早被提出的一类3D点云分割模型 (Lidar Segmentation)，作为点云目标检测的先驱和奠基。</p> <h3 id="算法摘要">算法摘要</h3> <p>由斯坦福大学于 2016 年发表论文 “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation”，是点云神经网络的鼻祖，它提出了一种网络结构，可以直接从点云中学习特征。</p> <p>该文章在分类、语义分割两种任务上做出了对比，并给了理论和实验分析。点云的特点其实非常好理解，只要网络抓住以下三个特点，那么它至少就能作为一个能用的 encoder 。</p> <ol> <li>排列不变性：重排一遍所有点的输入顺序，所表示的还是同一个点云数据，网络的输出应该相同。</li> <li>点集之间的交互性：点与点之间有未知的关联性。</li> <li>变换不变性：对于某些变换，例如仿射变换，应用在点云上时，不应该改变网络对点云的理解。</li> </ol> <h3 id="创新点">创新点</h3> <p>基于以上提到的点云数据三个特点，PointNet 对问题的处理是：</p> <ol> <li>排列不变性：该文章使用了对称函数（Symmetry Function），它的特点是对输入集合的顺序不敏感。这种函数非常多，如 maxpooling，加法，向量点积等。PointNet 采用的是 maxpooling 方法来聚合点集信息。</li> <li>点集之间的交互性：实际上，对称函数的聚合操作就已经得到了全局的信息，此时将点的特征向量与全局的特征向量 concat 起来，就可以让每个点感知到全局的语义信息了，即所谓的交互性。</li> <li>变换不变性：只需要对输入做一个标准化操作即可。PointNet 使用网络训练出了 D 维空间的变换矩阵。</li> </ol> <h3 id="网络结构">网络结构</h3> <p>PointNet 网络分为两个部分：分类网络 (Classifiction Network) 和分割网络 (Segmentation Network)。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/pointnet-structure-480.webp 480w, /assets/img/blog/pcd-survey/pointnet-structure-800.webp 800w, /assets/img/blog/pcd-survey/pointnet-structure-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/pointnet-structure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PointNet算法网络结构 </div> <h4 id="分类网络">分类网络</h4> <p>基本思路：分类网络以 \(n\) 个点作为输入，对输入做特征变换（变换矩阵），将结果输入给 MLP 做回归。将上一步的结果放在特征空间中做特征变换，再输入给 MLP 回归，对得到的输出进行 Maxpool 挑选最大值，作为整个 3D 点云的全局特征。 分类网络设计 Gloabl Feature，这一步称为 Symmetry Function for Unordered Input，对输入做处理。具体来讲，用一个简单的对称函数聚集每个点的信息：</p> \[f(\{x_1,...,x_2\}) \approx g(h(x_1),...,h(x_n))\] <p>对此过程数学建模，\(f\)为目标，$g$ 为设计的对称函数。从公式来看，其基本思路是：对各个点\(x_k\)分别做$h$处理，再将所有处理后的点交由函数$g$处理，以实现排列不变性。在实现中，\(h\)为 MLP，\(g\)为 maxpooling。</p> <h4 id="分割网络">分割网络</h4> <p>基本思路：分割网络将经过特征空间变换后的点局部特征 (local) 与全局特征 (global) 拼接，输入给 MLP 处理，对每一个点进行分类。</p> <p>分割网络获取 Point-wise Feature，这一步称为 Local and Global Information Aggregation，对两个不同维度的特征做拼接。</p> <p>但由于特征空间中的变换矩阵维度远远大于空间中的变换矩阵维度，在softmax训来时，用一个正则化项，将特这个变换矩阵限制为近似的正交矩阵，即输出尺度归一化。这一步称为 Joint Alignment Network：</p> \[L_{reg}={||I-AA^T||}^2_F\] <p>其中\(A\)是维度较小网络预测的特征对齐矩阵。消融实验证明该步骤可以有效优化，使模型效果更稳定。</p> <h3 id="模型效果">模型效果</h3> <p>数据援引论文原文，指标为点的 mIoU(\%)。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/blog/pcd-survey/pointnet%E6%95%88%E6%9E%9C-480.webp 480w, /assets/img/blog/pcd-survey/pointnet%E6%95%88%E6%9E%9C-800.webp 800w, /assets/img/blog/pcd-survey/pointnet%E6%95%88%E6%9E%9C-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/pcd-survey/pointnet%E6%95%88%E6%9E%9C.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PointNet算法效果 </div> <h3 id="总结">总结</h3> <p>PointNet之所以影响力巨大，并不仅仅是因为它是第一篇点云目标检测文章，更重要的是它的网络很简洁（简洁中蕴含了大量的工作来探寻出简洁这条路）却非常的work，这也就使得它能够成为一个工具，一个为点云表征的encoder工具，应用到更广阔的点云处理任务中。</p> <p>仅用 MLP+max pooling 就击败了众多SOTA，令人惊讶。另外PointNet在众多细节设计也都进行了理论分析和消融实验验证，保证了严谨性，这也为PointNet后面能够大规模被应用提供了支持。</p> <p>由于 PointNet 模型只使用了 MLP 和 Maxpooling，所获得的特征是全局的，没有捕获局部结构特在，在细节处理和泛用性都不是很好。为使得特征更关注于“局部”，对 3D 点云进行有重叠的多次降采样，分别对每次采样做特征提取，最后进行拼接，其余思路和 PointNet 类似。</p>]]></content><author><name></name></author><category term="selfstudy-note"/><category term="AD"/><summary type="html"><![CDATA[I have no experience translating it into English and publishing it.]]></summary></entry></feed>